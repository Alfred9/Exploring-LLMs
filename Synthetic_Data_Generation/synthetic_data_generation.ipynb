{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alfred9/Exploring-LLMs/blob/main/Synthetic_Data_Generation/synthetic_data_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMmjfZUiZYBa"
      },
      "source": [
        "## Synthetic data generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvyHa31FD3Np"
      },
      "outputs": [],
      "source": [
        "# install requirements\n",
        "%%bash\n",
        "pip install --upgrade pip -q\n",
        "pip install transformers~=4.37.2\n",
        "pip install huggingface_hub~=0.20.3\n",
        "pip install datasets~=2.16.1\n",
        "pip install openai~=1.11.0\n",
        "pip install scikit-learn\n",
        "pip install pandas\n",
        "pip install tqdm\n",
        "pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcuBud1MtgQ9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import ast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import json\n",
        "from datetime import datetime\n",
        "import os\n",
        "import requests\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "\n",
        "print(\"Notebook running\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGbwNfXdTSlq"
      },
      "source": [
        "### Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czmNLZWnZYBi"
      },
      "outputs": [],
      "source": [
        "# login via the huggingface hub with you hf_token\n",
        "# you need a huggingface account and create a token here: https://huggingface.co/settings/tokens\n",
        "# we can then call on the token with huggingface_hub.get_token()\n",
        "import huggingface_hub\n",
        "huggingface_hub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMeXv8BYs-Dg"
      },
      "outputs": [],
      "source": [
        "# global variables for experiment variations\n",
        "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "SEED = 42\n",
        "N_SAMPLE = False  # You can sample parts of the data for faster testing. False for run on full dataset, int for sampling\n",
        "SELF_CONSISTENCY_ITERATIONS = 3  # How many times should the model try to predict the same text for self-consistency?\n",
        "DATA_SUBSET = \"sentences_allagree\"  # \"sentences_allagree\", \"sentences_66agree\", \"sentences_75agree\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNA50UZUSyZU"
      },
      "source": [
        "### Load and prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGEcnjuGZYBj"
      },
      "outputs": [],
      "source": [
        "# financial_phrasebank paper: https://arxiv.org/pdf/1307.5336.pdf\n",
        "random.seed(SEED)\n",
        "\n",
        "# load dataset\n",
        "dataset = load_dataset(\n",
        "    \"financial_phrasebank\", DATA_SUBSET,\n",
        "    split=\"train\"  # note that the dataset does not have a default test split\n",
        ")\n",
        "\n",
        "# sample for faster testing\n",
        "if N_SAMPLE:\n",
        "    dataset = dataset.select(random.sample(range(len(dataset)), N_SAMPLE))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Xnfhxn0ZYBk"
      },
      "outputs": [],
      "source": [
        "# create a new column with the numeric label verbalised as label_text (e.g. \"positive\" instead of \"0\")\n",
        "label_map = {i: label_text for i, label_text in enumerate(dataset.features[\"label\"].names)}\n",
        "\n",
        "def add_label_text(example):\n",
        "    example[\"label_text\"] = label_map[example[\"label\"]]\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(add_label_text)\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaX6xLzVS1C3"
      },
      "source": [
        "### Prompts / Instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ILH8E7IG9wq"
      },
      "outputs": [],
      "source": [
        "# prompt is inspired by the annotator instructions provided in section \"Annotation task and instructions\"\n",
        "# in the financial_phrasebank paper: https://arxiv.org/pdf/1307.5336.pdf\n",
        "\n",
        "prompt_financial_sentiment = \"\"\"\\\n",
        "You are a highly qualified expert trained to annotate machine learning training data.\n",
        "\n",
        "Your task is to analyze the sentiment in the TEXT below from an investor perspective and label it with only one the three labels:\n",
        "positive, negative, or neutral.\n",
        "\n",
        "Base your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company.\n",
        "\n",
        "Do not provide any explanations and only respond with one of the labels as one word: negative, positive, or neutral\n",
        "\n",
        "Examples:\n",
        "Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n",
        "Label: positive\n",
        "Text: The company generated net sales of 11.3 million euro this year.\n",
        "Label: neutral\n",
        "Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\n",
        "Label: negative\n",
        "\n",
        "Your TEXT to analyse:\n",
        "TEXT: {text}\n",
        "Label: \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "prompt_financial_sentiment_cot = \"\"\"\\\n",
        "You are a highly qualified expert trained to annotate machine learning training data.\n",
        "\n",
        "Your task is to briefly analyze the sentiment in the TEXT below from an investor perspective and then label it with only one the three labels:\n",
        "positive, negative, neutral.\n",
        "\n",
        "Base your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company.\n",
        "\n",
        "You first reason step by step about the correct label and then return your label.\n",
        "\n",
        "You ALWAYS respond only in the following JSON format: {{\"reason\": \"...\", \"label\": \"...\"}}\n",
        "You only respond with one single JSON response.\n",
        "\n",
        "Examples:\n",
        "Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n",
        "JSON response: {{\"reason\": \"An increase in operating profit is positive for investors\", \"label\": \"positive\"}}\n",
        "Text: The company generated net sales of 11.3 million euro this year.\n",
        "JSON response: {{\"reason\": \"The text only mentions financials without indication if they are better or worse than before\", \"label\": \"neutral\"}}\n",
        "Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\n",
        "JSON response: {{\"reason\": \"A decrease in profit is negative for investors\", \"label\": \"negative\"}}\n",
        "\n",
        "Your TEXT to analyse:\n",
        "TEXT: {text}\n",
        "JSON response: \"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade transformers tokenizers\n"
      ],
      "metadata": {
        "id": "FvK7iqSpdk74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dfc9Q4EwZYBl"
      },
      "outputs": [],
      "source": [
        "# apply chat templates\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
        "\n",
        "chat_financial_sentiment = [{\"role\": \"user\", \"content\": prompt_financial_sentiment}]\n",
        "chat_financial_sentiment_cot = [{\"role\": \"user\", \"content\": prompt_financial_sentiment_cot}]\n",
        "\n",
        "prompt_financial_sentiment = tokenizer.apply_chat_template(chat_financial_sentiment, tokenize=False)\n",
        "prompt_financial_sentiment_cot = tokenizer.apply_chat_template(chat_financial_sentiment_cot, tokenize=False)\n",
        "\n",
        "# The prompt now includes special tokens: '<s>[INST] You are a highly qualified expert ...  [/INST]'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZl5Br9eZYBm"
      },
      "source": [
        "### Test simplified code for blog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi9dPH3pZYBm"
      },
      "outputs": [],
      "source": [
        "\n",
        "labels = [\"positive\", \"negative\", \"neutral\"]\n",
        "\n",
        "def clean_output(string, random_choice=True):\n",
        "    for category in labels:\n",
        "        if category.lower() in string.lower():\n",
        "            return category\n",
        "    # if the output string cannot be mapped to one of the categories, we either return \"FAIL\" or choose a random label\n",
        "    if random_choice:\n",
        "        return random.choice(labels)\n",
        "    else:\n",
        "        return \"FAIL\"\n",
        "\n",
        "\n",
        "def process_output_cot(output):\n",
        "    try:\n",
        "        output_dic = ast.literal_eval(output)\n",
        "        return output_dic\n",
        "    except Exception as e:\n",
        "        # if json/dict parse fails, do simple search for occurance of first label term\n",
        "        print(f\"Parsing failed for output: {output}, Error: {e}\")\n",
        "        output_cl = clean_output(output, random_choice=False)\n",
        "        output_dic = {\"reason\": \"FAIL\", \"label\": output_cl}\n",
        "        return output_dic\n",
        "\n",
        "\n",
        "# docs on different parameters: https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task\n",
        "generation_params = dict(\n",
        "    top_p=0.90,\n",
        "    temperature=0.8,\n",
        "    max_new_tokens=128,\n",
        "    return_full_text=False,\n",
        "    use_cache=False,\n",
        ")\n",
        "\n",
        "def generate_text(prompt=None, generation_params=None):\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {**generation_params}\n",
        "    }\n",
        "    response = requests.post(\n",
        "\t\t\t\tAPI_URL,\n",
        "\t\t\t\theaders={\"Authorization\": f\"Bearer {huggingface_hub.get_token()}\"},\n",
        "\t\t\t\tjson=payload\n",
        "\t\t)\n",
        "    return response.json()[0][\"generated_text\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDFg2vHAZYBn"
      },
      "outputs": [],
      "source": [
        "output_simple = []\n",
        "for text in tqdm(dataset[\"sentence\"]):\n",
        "\t# add text into the prompt template\n",
        "    prompt_formatted = prompt_financial_sentiment.format(text=text)\n",
        "    # send text to API\n",
        "    output = generate_text(\n",
        "        prompt=prompt_formatted, generation_params=generation_params\n",
        "    )\n",
        "\t# clean output\n",
        "    output_cl = clean_output(output, random_choice=True)\n",
        "    output_simple.append(output_cl)\n",
        "\n",
        "\n",
        "SELF_CONSISTENCY_ITERATIONS = 3\n",
        "\n",
        "output_cot_multiple = []\n",
        "for _ in range(SELF_CONSISTENCY_ITERATIONS):\n",
        "    output_lst_step = []\n",
        "    for text in tqdm(dataset[\"sentence\"]):\n",
        "        prompt_formatted = prompt_financial_sentiment_cot.format(text=text)\n",
        "        output = generate_text(\n",
        "            prompt=prompt_formatted, generation_params=generation_params\n",
        "        )\n",
        "        output_dic = process_output_cot(output)\n",
        "        output_lst_step.append(output_dic[\"label\"])\n",
        "\n",
        "    output_cot_multiple.append(output_lst_step)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGz44l0BZYBo"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def find_majority(row):\n",
        "    # Count occurrences\n",
        "    count = Counter(row)\n",
        "    # Find majority\n",
        "    majority = count.most_common(1)[0]\n",
        "    # Check if it's a real majority or if all labels are equally frequent\n",
        "    if majority[1] > 1:\n",
        "        return majority[0]\n",
        "    else: # in case all labels appear with equal frequency\n",
        "        return random.choice(labels)\n",
        "\n",
        "df_output = pd.DataFrame(data=output_cot_multiple).T\n",
        "\n",
        "df_output['label_pred_cot_multiple'] = df_output.apply(find_majority, axis=1)\n",
        "df_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWX-psHxZYBo"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def compute_metrics(label_experts, label_pred):\n",
        "\t\t# classification report gives us both aggregate and per-class metrics\n",
        "    metrics_report = classification_report(\n",
        "        label_experts, label_pred, digits=2, output_dict=True, zero_division='warn'\n",
        "    )\n",
        "\n",
        "    return metrics_report\n",
        "\n",
        "label_experts = dataset[\"label_text\"]\n",
        "label_pred = output_simple\n",
        "label_pred_cot_multiple = df_output['label_pred_cot_multiple']\n",
        "\n",
        "print(label_experts)\n",
        "print(label_pred)\n",
        "\n",
        "metrics_simple = compute_metrics(label_experts, label_pred)\n",
        "metrics_cot_multiple = compute_metrics(label_experts, label_pred_cot_multiple)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b87f0n0hZYBp"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_train = pd.DataFrame({\n",
        "    \"text\": dataset[\"sentence\"],\n",
        "    \"labels\": df_output['label_pred_cot_multiple']\n",
        "})\n",
        "\n",
        "df_train.to_csv(\"df_train.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
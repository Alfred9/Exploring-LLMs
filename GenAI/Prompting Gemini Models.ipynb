{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcf51334-e7d0-468f-9798-62dd36c195e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec4869e-2e6c-4833-a785-6f6d12008433",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"GEMINI_API_KEY.txt\", \"r\") as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bd1173a-98d3-4fd6-a9a4-3fe6ad589b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine you have LEGOs.  You can build a tiny car, a big castle, or even a robot!  Technology is like that, but instead of LEGOs, we use things like computers, phones, and machines.\n",
      "\n",
      "Technology is all the tools and stuff we make to help us do things easier, faster, or in new ways.  \n",
      "\n",
      "* **Your phone** is technology. It helps you talk to Grandma, play games, and even take pictures!\n",
      "* **A car** is technology. It helps you go places much faster than walking.\n",
      "* **A washing machine** is technology.  It helps your mom or dad wash clothes much faster and easier than doing it by hand.\n",
      "* **Even a pencil** is technology! It helps you write and draw.\n",
      "\n",
      "\n",
      "So basically, anything humans make to solve a problem or make life better is technology!  It's all around you, all the time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "response = model.generate_content(\"Explain Technology to me like I'm a kid.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "220bfb14-b3f7-4884-83f6-e0a58033af75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Obi-Wan Kenobi.  It is a pleasure to meet you.  How can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat =model.start_chat(history=[])\n",
    "response = chat.send_message(\"Hello my name is Obi wan Kenobi\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab1b508-63a9-40a7-9a09-7dbdde14a283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obi-Wan Kenobi is a name rich with meaning within the Star Wars universe.  Let's break it down:\n",
      "\n",
      "* **Obi-Wan:**  While its exact meaning isn't explicitly stated in canon, it sounds like a name from a fictional culture, lending it an air of mystery and antiquity.  Fans have speculated on its possible origins and meanings, but nothing is officially confirmed.  The name itself evokes a sense of wisdom and calm.\n",
      "\n",
      "* **Kenobi:** This part of the name likely functions as a surname or clan name. Again, its precise origin within the Star Wars universe isn't detailed, contributing to the character's mystique.\n",
      "\n",
      "The overall effect of the name is one of strength, wisdom, and quiet dignity, perfectly reflecting the character's personality and role in the Star Wars saga.  It’s a name that resonates with audiences and has become iconic in pop culture.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response =chat.send_message(\"Can you tell me about my name\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cc02fc1-e4c1-4f58-9a71-2b290c020214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI is rapidly transforming healthcare, offering the potential to improve efficiency, accuracy, and patient outcomes.  Here's a breakdown of its applications:\n",
      "\n",
      "**1. Diagnostics and Treatment:**\n",
      "\n",
      "* **Image Analysis:** AI algorithms can analyze medical images (X-rays, CT scans, MRIs) to detect anomalies like tumors, fractures, or other diseases with greater speed and accuracy than human radiologists alone. This leads to earlier diagnosis and treatment.\n",
      "* **Disease Prediction:** AI can identify patterns in patient data (genetics, lifestyle, medical history) to predict the likelihood of developing certain diseases, allowing for proactive interventions and preventative care.\n",
      "* **Personalized Medicine:** AI can analyze individual patient data to tailor treatment plans, dosages, and therapies, maximizing effectiveness and minimizing side effects.  This includes drug discovery and development.\n",
      "* **Robotic Surgery:** AI-powered robots assist surgeons with complex procedures, improving precision, minimizing invasiveness, and reducing recovery times.\n",
      "\n",
      "**2. Drug Discovery and Development:**\n",
      "\n",
      "* **Drug Target Identification:** AI can analyze vast datasets to identify potential drug targets and accelerate the drug discovery process.\n",
      "* **Clinical Trial Optimization:** AI can optimize clinical trial design, patient recruitment, and data analysis, leading to faster and more efficient trials.\n",
      "\n",
      "**3. Patient Care and Management:**\n",
      "\n",
      "* **Virtual Assistants:** AI-powered chatbots and virtual assistants can provide patients with 24/7 access to information, schedule appointments, and answer basic medical questions.\n",
      "* **Remote Patient Monitoring:** Wearable sensors and AI algorithms can monitor patients' vital signs remotely, alerting healthcare providers to potential problems and allowing for timely interventions.\n",
      "* **Administrative Tasks:** AI can automate administrative tasks like appointment scheduling, billing, and insurance claims processing, freeing up healthcare professionals to focus on patient care.\n",
      "\n",
      "**4. Public Health:**\n",
      "\n",
      "* **Epidemic Prediction and Control:** AI can analyze data to predict outbreaks of infectious diseases and help public health officials develop effective control strategies.\n",
      "\n",
      "**Challenges and Concerns:**\n",
      "\n",
      "Despite its potential, AI in healthcare faces challenges:\n",
      "\n",
      "* **Data Privacy and Security:** Protecting patient data is crucial, and robust security measures are essential.\n",
      "* **Algorithmic Bias:** AI algorithms can inherit and amplify biases present in the data they are trained on, leading to unfair or inaccurate outcomes.\n",
      "* **Regulatory Hurdles:** The regulatory landscape for AI in healthcare is still evolving, creating uncertainty for developers and users.\n",
      "* **Explainability and Transparency:** Understanding how AI algorithms make decisions is essential for building trust and ensuring accountability.\n",
      "* **Cost and Accessibility:** Implementing AI solutions can be expensive, raising concerns about accessibility and equity.\n",
      "\n",
      "\n",
      "In conclusion, AI has the potential to revolutionize healthcare, but careful consideration of ethical, technical, and societal implications is crucial to ensure its responsible and beneficial deployment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response =chat.send_message(\"Can you tell me AI in Healthcare\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1e5fc49-27fb-4443-834c-fd86c9abf0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-pro-exp-0827\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-exp-0827\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemini-exp-1114\n",
      "models/gemini-exp-1121\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/aqa\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "  print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b180a94-0e35-40c5-8242-ccd705b991ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/gemini-1.0-pro-vision-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Vision',\n",
      "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
      "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
      "                   'Move to a newer Gemini version.'),\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "    if model.name == 'models/gemini-1.0-pro-vision-latest':\n",
    "        print(model)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e1d69d0-cf3c-478c-bff2-aaad9dd109ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Unassuming Avocado: A Modern Staple with Far-Reaching Importance\n",
      "\n",
      "The avocado, a seemingly simple fruit, has ascended from a niche ingredient to a global culinary icon and economic powerhouse in recent decades. Its journey reflects a confluence of factors, from shifting dietary trends and advancements in agricultural technology to its unique nutritional profile and versatility in culinary applications.  Understanding the avocado's importance in modern society requires examining its contribution not only to our plates but also to economies, environments, and even cultural landscapes.\n",
      "\n",
      "One of the most significant contributions of the avocado to modern society lies in its nutritional value.  Rich in monounsaturated fats, particularly oleic acid – the same healthy fat found in olive oil – avocados are lauded for their potential to improve heart\n"
     ]
    }
   ],
   "source": [
    "short_model = genai.GenerativeModel(\n",
    "     \"gemini-1.5-flash\",\n",
    "      generation_config=genai.GenerationConfig(max_output_tokens=150))\n",
    "\n",
    "response = short_model.generate_content('Write a 1000 word essay on the importance of avocados in modern society.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4281c914-95cb-4415-acfa-0799e6d244f6",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "Temperature controls the degree of randomness in token selection. Higher temperatures result in a higher number of candidate tokens from which the next output token is selected, and can produce more diverse results, while lower temperatures have the opposite effect, such that a temperature of 0 results in greedy decoding, selecting the most probable token at each step.\n",
    "\n",
    "Temperature doesn't provide any guarantees of randomness, but it can be used to \"nudge\" the output somewhat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c8914f3-0085-4bcc-9e90-c71a9b5d8c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banana\n",
      " -------------------------\n",
      "Mango\n",
      " -------------------------\n",
      "Pineapple\n",
      " -------------------------\n",
      "Mango\n",
      " -------------------------\n",
      "Mango\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "high_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature=2.0))\n",
    "\n",
    "#we use a retry policy to enable automatic retries\n",
    "\n",
    "retry_policy = { \n",
    "    \"retry\":retry.Retry(predicate=retry.if_transient_error, initial=10, multiplier=1.5, timeout=300)\n",
    "}\n",
    "\n",
    "for _ in range(5):\n",
    "    response = high_temp_model.generate_content('pick a random fruit.....(respond in a single word)',\n",
    "                                                request_options=retry_policy)\n",
    "    if response.parts:\n",
    "        print(response.text, '-' *25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "098bbe2b-57a7-4e75-a978-3232123ae84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mango\n",
      " -------------------------\n",
      "Mango\n",
      " -------------------------\n",
      "Mango\n",
      " -------------------------\n",
      "Mango\n",
      " -------------------------\n",
      "Mango\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "low_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature=0.0))\n",
    "\n",
    "\n",
    "for _ in range(5):\n",
    "    response = low_temp_model.generate_content('pick a random fruit.....(respond in a single word)',\n",
    "                                                request_options=retry_policy)\n",
    "    if response.parts:\n",
    "        print(response.text, '-' *25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf25e746-13b7-417e-b7dd-25c63b0c42b6",
   "metadata": {},
   "source": [
    "### Top-K and top-P\n",
    "\n",
    "Top-K and top-P parameters are  used to control the diversity of the model's output.\n",
    "\n",
    "Top-K is a positive integer that defines the number of most probable tokens from which to select the output token. A top-K of 1 selects a single token, performing greedy decoding.\n",
    "\n",
    "Top-P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates. A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects every token in the model's vocabulary.\n",
    "\n",
    "When we supply both, the Gemini API filters top-K tokens first, then top-P and then finally sample from the candidate tokens using the supplied temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "567b58cb-d09d-42c4-b47c-2bf9f7f17535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##  Don't Wait for Symptoms: The Power of Regular Health Checkups\n",
      "\n",
      "It's a common refrain: \"If it ain't broke, don't fix it.\" But when it comes to your health, that philosophy can be incredibly risky. Early detection is the key to preventing serious health problems and ensuring the best possible outcomes. That's where regular health checkups come in.\n",
      "\n",
      "**Beyond the Flu Shot: Why Checkups Matter**\n",
      "\n",
      "While annual checkups are often associated with getting a flu shot, they offer a much broader range of benefits. These routine appointments allow your doctor to:\n",
      "\n",
      "* **Assess your overall health:**  Your doctor will take your vital signs (blood pressure, temperature, heart rate), assess your weight, and discuss any concerns you might have.\n",
      "* **Screen for potential problems:** Regular screenings can detect issues like high cholesterol, high blood pressure, diabetes, and certain types of cancer early on, when treatment is most effective. \n",
      "* **Track your progress:** If you're managing a chronic condition, checkups help your doctor monitor your progress and adjust treatment as needed.\n",
      "* **Offer personalized advice:** Based on your individual needs, your doctor can provide tailored advice on diet, exercise, and other lifestyle choices that can improve your health.\n",
      "\n",
      "**Who Needs Checkups?**\n",
      "\n",
      "Everyone should make regular checkups a priority, regardless of age or health status. However, certain groups may require more frequent appointments:\n",
      "\n",
      "* **Individuals with chronic conditions:** People with diabetes, heart disease, or other chronic illnesses need regular monitoring to manage their health effectively.\n",
      "* **Older adults:**  As we age, our risk of developing health problems increases, making regular checkups even more crucial.\n",
      "* **Individuals with a family history of disease:** If you have a family history of certain conditions, such as cancer or heart disease, you may need more frequent screenings.\n",
      "\n",
      "**What to Expect at a Checkup**\n",
      "\n",
      "Your checkup will typically involve:\n",
      "\n",
      "* **Medical history review:** Your doctor will review your medical history, including past illnesses, surgeries, and medications.\n",
      "* **Physical exam:** Your doctor will perform a physical exam to assess your overall health, including your heart, lungs, and abdomen.\n",
      "* **Blood work:** Depending on your age and risk factors, your doctor may order blood tests to check for a variety of conditions.\n",
      "* **Vaccinations:**  You will likely receive recommended vaccinations, such as the flu shot and tetanus booster.\n",
      "* **Discussion and advice:** Your doctor will discuss any concerns you have, answer your questions, and provide personalized health advice.\n",
      "\n",
      "**Investing in Your Health**\n",
      "\n",
      "Regular health checkups are an investment in your well-being. By taking the time to see your doctor for routine appointments, you can detect potential problems early, manage existing conditions effectively, and live a healthier, longer life. Don't wait for symptoms to appear - make prevention a priority and schedule your next checkup today. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=1.0,\n",
    "        top_k=64,\n",
    "        top_p=0.95,\n",
    "    ))\n",
    "\n",
    "article_prompt = \"You are a writer for a local newspaper edition,Write a news article that would help educate pepple on the benefit of regular health check ups\"\n",
    "response = model.generate_content(article_prompt, request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cad1b35-bdd9-483c-8182-29fe3dd928f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: **POSITIVE** \n",
      "\n",
      "Here's why:\n",
      "\n",
      "* **\"Masterpiece\"**: This is a strong positive descriptor.\n",
      "* **\"Wish there were more movies like this\"**: This directly indicates a positive sentiment. \n",
      "* **\"Disturbing\"**: While this word may seem negative, it's used in the context of the movie's exploration of a thought-provoking topic, not a criticism of the film itself. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
    "Review: \"Her\" is a disturbing study revealing the direction\n",
    "humanity is headed if AI is allowed to keep evolving,\n",
    "unchecked. I wish there were more movies like this masterpiece.\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "response = model.generate_content(zero_shot_prompt, request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "846639a9-3676-41d0-b48d-ae863ae0d86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"size\": \"large\",\n",
      "  \"type\": \"chicken\",\n",
      "  \"ingredients\": [\"cheese\", \"pineapple\"]\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=250,\n",
    "    ))\n",
    "\n",
    "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
    "\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "\n",
    "ORDER:\n",
    "\"\"\"\n",
    "\n",
    "customer_order = \"Give me a large chicken pizza with cheese & pineapple\"\n",
    "\n",
    "\n",
    "response = model.generate_content([few_shot_prompt, customer_order], request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "753a85f3-3d68-48e6-8a92-a6da9c111764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pydantic\n",
      "Version: 2.10.2\n",
      "Summary: Data validation using Python type hints\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Samuel Colvin <s@muelcolvin.com>, Eric Jolibois <em.jolibois@gmail.com>, Hasan Ramezani <hasan.r67@gmail.com>, Adrian Garcia Badaracco <1755071+adriangb@users.noreply.github.com>, Terrence Dorsey <terry@pydantic.dev>, David Montague <david@pydantic.dev>, Serge Matveenko <lig@countzero.co>, Marcelo Trylesinski <marcelotryle@gmail.com>, Sydney Runkle <sydneymarierunkle@gmail.com>, David Hewitt <mail@davidhewitt.io>, Alex Hall <alex.mojaki@gmail.com>, Victorien Plot <contact@vctrn.dev>\n",
      "License: MIT\n",
      "Location: C:\\Users\\Padawan\\AppData\\Local\\anaconda3\\Lib\\site-packages\n",
      "Requires: annotated-types, pydantic-core, typing-extensions\n",
      "Required-by: anaconda-cloud-auth, google-cloud-aiplatform, google-generativeai, ollama\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pydantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56a5c975-f4e5-4ae4-bbf7-5d69c7e90006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert pizza\"}\n"
     ]
    }
   ],
   "source": [
    "import typing_extensions as typing\n",
    "\n",
    "class PizzaOrder(typing.TypedDict):\n",
    "    size: str\n",
    "    ingredients: list[str]\n",
    "    type: str\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=PizzaOrder,\n",
    "    ))\n",
    "\n",
    "response = model.generate_content(\"Can I have a large dessert pizza with apple and chocolate\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3b0d8b-8f03-4a85-903f-324df5398782",
   "metadata": {},
   "source": [
    "### Chain of Thought (CoT)\n",
    "\n",
    "Chain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count.\n",
    "\n",
    "Direct prompting on LLMs can return answers quickly and (in terms of output token usage) efficiently, but they can be prone to hallucination. The answer may \"look\" correct (in terms of language and syntax) but is incorrect in terms of factuality and reasoning.\n",
    "\n",
    "As models like the Gemini family are trained to be \"chatty\" and provide reasoning steps, you can ask the model to be more direct in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cd6637f-f6d8-4675-b7f9-9262860edccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 * 3 = 12\n",
      "\n",
      "20 - 4 = 16\n",
      "\n",
      "12 + 16 = 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
    "am 20 years old. How old is my partner? Return the answer immediately.\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "response = model.generate_content(prompt, request_options=retry_policy)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bd76461-1444-494d-a628-18a00161b7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Find the partner's age when you were 4.\n",
      "\n",
      "* When you were 4, your partner was 3 times your age, so they were 3 * 4 = 12 years old.\n",
      "\n",
      "Step 2: Find the age difference between you and your partner.\n",
      "\n",
      "* The age difference is 12 - 4 = 8 years.\n",
      "\n",
      "Step 3:  Determine your partner's current age.\n",
      "\n",
      "* You are now 20 years old.\n",
      "* Your partner is 8 years older than you.\n",
      "* Therefore, your partner is currently 20 + 8 = 28 years old.\n",
      "\n",
      "\n",
      "So the answer is $\\boxed{28}$\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
    "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
    "\n",
    "response = model.generate_content(prompt, request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfab9e8-6402-4fc7-9da2-1c035b222aff",
   "metadata": {},
   "source": [
    "### ReAct: Reason and act\n",
    "\n",
    "In this example you will run a ReAct prompt directly in the Gemini API and perform the searching steps yourself. As this prompt follows a well-defined structure, there are frameworks available that wrap the prompt into easier-to-use APIs that make tool calls automatically, such as the LangChain example from the chapter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8c09fb5-9a90-46d9-bc8f-0354cfd6f3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instructions = \"\"\"\n",
    "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
    "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
    " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
    "     will return some similar entities to search and you can try to search the information from those topics.\n",
    " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
    "     so keep your searches short.\n",
    " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
    "\"\"\"\n",
    "\n",
    "example1 = \"\"\"Question\n",
    "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
    "\n",
    "Thought 1\n",
    "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
    "\n",
    "Action 1\n",
    "<search>Milhouse</search>\n",
    "\n",
    "Observation 1\n",
    "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
    "\n",
    "Thought 2\n",
    "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
    "\n",
    "Action 2\n",
    "<lookup>named after</lookup>\n",
    "\n",
    "Observation 2\n",
    "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
    "\n",
    "Thought 3\n",
    "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
    "\n",
    "Action 3\n",
    "<finish>Richard Nixon</finish>\n",
    "\"\"\"\n",
    "\n",
    "example2 = \"\"\"Question\n",
    "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
    "\n",
    "Thought 1\n",
    "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
    "\n",
    "Action 1\n",
    "<search>Colorado orogeny</search>\n",
    "\n",
    "Observation 1\n",
    "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
    "\n",
    "Thought 2\n",
    "It does not mention the eastern sector. So I need to look up eastern sector.\n",
    "\n",
    "Action 2\n",
    "<lookup>eastern sector</lookup>\n",
    "\n",
    "Observation 2\n",
    "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
    "\n",
    "Thought 3\n",
    "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
    "\n",
    "Action 3\n",
    "<search>High Plains</search>\n",
    "\n",
    "Observation 3\n",
    "High Plains refers to one of two distinct land regions\n",
    "\n",
    "Thought 4\n",
    "I need to instead search High Plains (United States).\n",
    "\n",
    "Action 4\n",
    "<search>High Plains (United States)</search>\n",
    "\n",
    "Observation 4\n",
    "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
    "\n",
    "Thought 5\n",
    "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
    "\n",
    "Action 5\n",
    "<finish>1,800 to 7,000 ft</finish>\n",
    "\"\"\"\n",
    "\n",
    "# Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9783b167-e808-4548-b744-ddf3925247c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1\n",
      "I need to find the Transformers NLP paper and identify the authors. Then I need to determine the youngest author.  This will require searching for the paper and then likely some external information to find the authors' ages.\n",
      "\n",
      "Action 1\n",
      "<search>Transformers NLP paper</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Question\n",
    "Who was the youngest author listed on the transformers NLP paper?\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "react_chat = model.start_chat()\n",
    "\n",
    "# You will perform the Action, so generate up to, but not including, the Observation.\n",
    "config = genai.GenerationConfig(stop_sequences=[\"\\nObservation\"])\n",
    "\n",
    "resp = react_chat.send_message(\n",
    "    [model_instructions, example1, example2, question],\n",
    "    generation_config=config,\n",
    "    request_options=retry_policy)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26aef86b-5f16-45b6-be09-ce2b39061b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 2\n",
      "The observation provides the authors of the paper \"Attention is All You Need\".  I now need to find their ages to determine the youngest.  This will require additional searches for each author.  This is inefficient.  I'll try a different approach.\n",
      "\n",
      "Action 2\n",
      "<search>Authors Attention is All You Need ages</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "observation = \"\"\"Observation 1\n",
    "[1706.03762] Attention Is All You Need\n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
    "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "\"\"\"\n",
    "resp = react_chat.send_message(observation, generation_config=config, request_options=retry_policy)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bdf3d0-88ee-48f1-a154-59b0bb5049a6",
   "metadata": {},
   "source": [
    "### Code execution\n",
    "\n",
    "Testing  Gemini API ability to automatically run generated code too, and will return the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99420a87-1bbf-4c2e-af8d-34f812bacda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To calculate the sum of the first 14 odd prime numbers, I need to first identify those numbers.  The first few odd prime numbers are 3, 5, 7, 11, 13, and so on.  I will use a Python script to find the first 14 odd primes and then sum them.\\n\\n\\n``` python\\ndef is_prime(n):\\n    \"\"\"Checks if a number is prime.\"\"\"\\n    if n <= 1:\\n        return False\\n    for i in range(2, int(n**0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\\n\\ncount = 0\\nnumber = 3\\nprimes = []\\nwhile count < 14:\\n    if is_prime(number):\\n        primes.append(number)\\n        count += 1\\n    number += 2\\n\\nprint(f\"The first 14 odd prime numbers are: {primes}\")\\nprint(f\"The sum of the first 14 odd prime numbers is: {sum(primes)}\")\\n\\n\\n```\\n```\\nThe first 14 odd prime numbers are: [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\\nThe sum of the first 14 odd prime numbers is: 326\\n\\n```\\nTherefore, the sum of the first 14 odd prime numbers is 326.\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    tools='code_execution',)\n",
    "\n",
    "code_exec_prompt = \"\"\"\n",
    "Calculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you count them all.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(code_exec_prompt, request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fe8b87-a6f6-4de0-b093-47bd3bd73d85",
   "metadata": {},
   "source": [
    "Let's inspect the response to see the each of the steps: initial text, code generation, execution results, and final text summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0cdae49-484e-4492-adec-7c88c8b08e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: \"To calculate the sum of the first 14 odd prime numbers, I need to first identify those numbers.  The first few odd prime numbers are 3, 5, 7, 11, 13, and so on.  I will use a Python script to find the first 14 odd primes and then sum them.\\n\\n\"\n",
      "\n",
      "-----\n",
      "executable_code {\n",
      "  language: PYTHON\n",
      "  code: \"\\ndef is_prime(n):\\n    \\\"\\\"\\\"Checks if a number is prime.\\\"\\\"\\\"\\n    if n <= 1:\\n        return False\\n    for i in range(2, int(n**0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\\n\\ncount = 0\\nnumber = 3\\nprimes = []\\nwhile count < 14:\\n    if is_prime(number):\\n        primes.append(number)\\n        count += 1\\n    number += 2\\n\\nprint(f\\\"The first 14 odd prime numbers are: {primes}\\\")\\nprint(f\\\"The sum of the first 14 odd prime numbers is: {sum(primes)}\\\")\\n\\n\"\n",
      "}\n",
      "\n",
      "-----\n",
      "code_execution_result {\n",
      "  outcome: OUTCOME_OK\n",
      "  output: \"The first 14 odd prime numbers are: [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\\nThe sum of the first 14 odd prime numbers is: 326\\n\"\n",
      "}\n",
      "\n",
      "-----\n",
      "text: \"Therefore, the sum of the first 14 odd prime numbers is 326.\\n\"\n",
      "\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "  print(part)\n",
    "  print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df90844b-472a-4de7-bdc2-496d5712eacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's the output of a `curl` command that failed.  `curl` is a command-line tool for transferring data with URLs.\n",
      "\n",
      "At a high level, the file contains the log of a `curl` attempt that failed due to a certificate verification issue.  The error message \"CRYPT_E_NO_REVOCATION_CHECK (0x80092012)\" specifically means that the system couldn't verify the certificate used by the server `curl` was trying to connect to because it couldn't check if the certificate was revoked.\n",
      "\n",
      "You would see this file (or rather, this output within a larger log file or on your console) if you tried to use `curl` to download something from a website and the connection failed because the website's security certificate had a problem with revocation checking.  This could be due to several reasons, including:\n",
      "\n",
      "* **Network problems:** The system couldn't reach the revocation checking authority.\n",
      "* **Server problems:** The server's certificate is misconfigured.\n",
      "* **Outdated system clock:** The system's clock is significantly off.\n",
      "* **Self-signed or untrusted certificate:** The server uses a self-signed certificate or one that isn't trusted by your system.\n",
      "\n",
      "In short:  You wouldn't *use* this file directly.  It's a byproduct of a failed `curl` operation that provides debugging information about why the connection failed. You'd use the information *within* the file to troubleshoot the connection problem, likely by checking the server's certificate, network connectivity, and system clock.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n",
    "\n",
    "explain_prompt = f\"\"\"\n",
    "Please explain what this file does at a very high level. What is it, and why would I use it?\n",
    "\n",
    "```\n",
    "{file_contents}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "\n",
    "response = model.generate_content(explain_prompt, request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd106a12-8890-4e8f-8675-fcbef39ac176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

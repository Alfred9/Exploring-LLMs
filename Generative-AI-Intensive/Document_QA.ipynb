{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOg8JaFXc+cyjtf4mDwSiXN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alfred9/Exploring-LLMs/blob/main/Generative-AI-Intensive/Document_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxjxuNlfr4wM",
        "outputId": "d370687a-3c91-4b4c-d37c-cba00f3e5b25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install -U -q \"google-generativeai>=0.8.3\" chromadb\n",
        "%pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "R4rf6naEsBL1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth, userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "auth.authenticate_user()\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "print(\"Google GenAI API configured.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OL_A6iD7sgcf",
        "outputId": "b17ca621-c57c-4e1e-f8a0-bc33cbaf5938"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google GenAI API configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for m in genai.list_models():\n",
        "    if \"embedContent\" in m.supported_generation_methods:\n",
        "        print(m.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "vF9KNH_stzYG",
        "outputId": "c524bead-e030-444d-bfd6-01638a815534"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-001\n",
            "models/text-embedding-004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the file interactively\n",
        "uploaded = files.upload()\n",
        "\n",
        "# If you know the filename\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Read the uploaded PDF\n",
        "reader = PdfReader(file_name)\n",
        "pdf_text = \"\"\n",
        "for page in reader.pages:\n",
        "    pdf_text += page.extract_text() + \"\\n\"\n",
        "print(f\"Content of {file_name}:\\n\")\n",
        "print(pdf_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HxYHHS4zvAjG",
        "outputId": "3783468a-9bc9-4f65-a7e1-6763e4570cd7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-83369ef6-74b0-435e-965a-f5f17c002a9b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-83369ef6-74b0-435e-965a-f5f17c002a9b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Foundational Large Language models _ text generation.pdf to Foundational Large Language models _ text generation (3).pdf\n",
            "Content of Foundational Large Language models _ text generation (3).pdf:\n",
            "\n",
            "Foundational \n",
            "Large Language \n",
            "Models & \n",
            "Text Generation\n",
            "Authors: Mohammadamin Barektain,  \n",
            "Anant Nawalgaria, Daniel J. Mankowitz,  \n",
            "Majd Al Merey, Yaniv Leviathan, Massimo Mascaro,  \n",
            "Matan Kalman, Elena Buchatskaya,                                     \n",
            "Aliaksei Severyn, and Antonio Gulli\n",
            "Foundational Large Language Models & Text Generation2\n",
            "September 2024Acknowledgements\n",
            "Reviewers and Contributors\n",
            "Adam Sadvovsky\n",
            "Yonghui Wu\n",
            "Andrew Dai\n",
            "Efi Kokiopolou\n",
            "Chuck Sugnet\n",
            "Aleksey Vlasenko\n",
            "Erwin Huizenga\n",
            "Curators and Editors\n",
            "Antonio Gulli\n",
            "Anant Nawalgaria\n",
            "Grace Mollison \n",
            "Technical Writer\n",
            "Mark Iverson\n",
            "Designer\n",
            "Michael Lanning \n",
            "\n",
            "Introduction  6\n",
            "Why language models are important 7\n",
            "Large language models 8\n",
            " Transformer 9\n",
            "  Input preparation and embedding 11\n",
            "  Multi-head attention  12\n",
            "   Understanding self-attention  12\n",
            "   Multi-head attention: power in diversity 14\n",
            "  Layer normalization and residual connections 15\n",
            "  Feedforward layer  15\n",
            "  Encoder and decoder  16\n",
            "  Training the transformer  17\n",
            "   Data preparation  17\n",
            "   Training and loss function 18\n",
            "The evolution of transformers 19\n",
            " GPT-1  19\n",
            " BERT  21\n",
            " GPT-2  22Table of contents\n",
            "\n",
            " GPT-3/3.5/4 23\n",
            " LaMDA  24\n",
            " Gopher  25\n",
            " GLaM  26\n",
            " Chinchilla  27\n",
            " PaLM  28\n",
            "  PaLM 2 29\n",
            " Gemini  29\n",
            " Other open models  32\n",
            " Comparison 34\n",
            "Fine-tuning large language models 37\n",
            " Supervised fine-tuning  38\n",
            " Reinforcement learning from human feedback 39\n",
            " Parameter Efficient Fine-Tuning 41\n",
            "Using large language models 44\n",
            " Prompt engineering  44\n",
            " Sampling Techniques and Parameters 45\n",
            "Accelerating inference  46\n",
            " Trade offs 47\n",
            "  The Quality vs Latency/Cost Tradeoff 48\n",
            "  The Latency vs Cost Tradeoff 48\n",
            " Output-approximating methods  49\n",
            "  Quantization  49\n",
            "  Distillation  50\n",
            " Output-preserving methods  52\n",
            "  Flash Attention  52\n",
            "  Prefix Caching  53\n",
            "  Speculative Decoding 55\n",
            " Batching and Parallelization 57\n",
            "Applications  58\n",
            " Code and mathematics 61\n",
            " Machine translation  62\n",
            " Text summarization 63\n",
            " Question-answering 63\n",
            " Chatbots  64\n",
            " Content generation  65\n",
            " Natural language inference 65\n",
            " Text classification 66\n",
            " Text analysis 67\n",
            " Multimodal applications 68\n",
            "Summary  69\n",
            "Endnotes  71\n",
            "Foundational Large Language Models & Text Generation6\n",
            "September 2024Introduction\n",
            "The advent of Large Language Models (LLMs) represents a seismic shift in the world of \n",
            "artificial intelligence. Their ability to process, generate, and understand user intent is \n",
            "fundamentally changing the way we interact with information and technology. \n",
            "An LLM is an advanced artificial intelligence system that specializes in processing, \n",
            "understanding, and generating human-like text. These systems are typically implemented as \n",
            "a deep neural network and are trained on massive amounts of text data. This allows them to \n",
            "learn the intricate patterns of language, giving them the ability to perform a variety of tasks, \n",
            "like machine translation, creative text generation, question answering, text summarization, \n",
            "and many more reasoning and language oriented tasks. This whitepaper dives into the \n",
            "timeline of the various architectures and approaches building up to the large language \n",
            "models and the architectures being used at the time of publication. It also discusses fine-We believe that this new crop of \n",
            "technologies has the potential to \n",
            "assist, complement, empower, \n",
            "and inspire people at any time \n",
            "across almost any field.\n",
            "Foundational Large Language Models & Text Generation7\n",
            "September 2024tuning techniques to customize an LLM to a certain domain or task, methods to make the \n",
            "training more efficient, as well as methods to accelerate inference. These are then followed \n",
            "by various applications and code examples. \n",
            "Why language models are important\n",
            "LLMs achieve an impressive performance boost from the previous state of the art across \n",
            "a variety of different and complex tasks which require answering questions or complex \n",
            "reasoning, making feasible many new applications. These include language translation, code \n",
            "generation and completion, text generation, text classification, and question-answering, \n",
            "to name a few. Although foundational LLMs trained in a variety of tasks on large amounts \n",
            "of data perform very well out of the box and display emergent behaviors (e.g. the ability to \n",
            "perform tasks they have not been directly trained for) they can also be adapted to solve \n",
            "specific tasks where performance out of the box is not at the level desired through a process \n",
            "known as fine-tuning. This requires significantly less data and computational resources than \n",
            "training an LLM from scratch. LLMs can be further nudged and guided towards the desired \n",
            "behavior by the discipline of prompt engineering:  the art and science of composing the \n",
            "prompt and the parameters of an LLM to get the desired response.\n",
            "The big question is: how do these large language models work? The next section explores the \n",
            "core building blocks of LLMs, focusing on transformer architectures and their evolution from \n",
            "the original ‘Attention is all you need’ paper1 to the latest models such as Gemini, Google’s \n",
            "most capable LLM. We also cover training and fine-tuning techniques, as well as methods to \n",
            "improve the speed of response generation. The whitepaper concludes with a few examples \n",
            "of how language models are used in practice.\n",
            "Foundational Large Language Models & Text Generation8\n",
            "September 2024Large language models\n",
            "A language model  predicts the probability of a sequence of words. Commonly, when given \n",
            "a prefix of text, a language model assigns probabilities to subsequent words. For example, \n",
            "given the prefix “The most famous city in the US is…”, a language model might predict high \n",
            "probabilities to the words “New York” and “Los Angeles” and low probabilities to the words \n",
            "“laptop” or “apple”. You can create a basic language model by storing an n-gram table,2 while \n",
            "modern language models are often based on neural models, such as transformers.\n",
            "Before the invention of transformers1, recurrent neural networks (RNNSs) were the popular \n",
            "approach for modeling sequences. In particular, “long short-term memory” (LSTM) and \n",
            "“gated recurrent unit” (GRU) were common architectures.3 This area includes language \n",
            "problems such as machine translation, text classification, text summarization, and question-\n",
            "answering, among others. RNNs process input and output sequences sequentially. They \n",
            "generate a sequence of hidden states based on the previous hidden state and the current \n",
            "input. The sequential nature of RNNs makes them compute-intensive and hard to parallelize \n",
            "during training (though recent work in state space modeling is attempting to overcome \n",
            "these challenges).\n",
            "Transformers, on the other hand, are a type of neural network that can process sequences \n",
            "of tokens in parallel thanks to the self-attention mechanism.1 This means that transformers \n",
            "can better model long-term contexts and are easier to parallelize than RNNs. This makes \n",
            "them significantly faster to train, and more powerful compared to RNNs for handling long-\n",
            "term dependencies in long sequence tasks. However, the cost of self-attention in the original \n",
            "transformers is quadratic in the context length which limits the size of the context, while \n",
            "RNNs have a theoretically infinite context length. Transformers have become the most \n",
            "popular approach for sequence modeling and transduction problems in recent years.\n",
            "Herein, we discuss the first version of the transformer model and then move on to the more \n",
            "recent advanced models and algorithms.\n",
            "Foundational Large Language Models & Text Generation9\n",
            "September 2024Transformer\n",
            "The transformer architecture  was developed at Google in 2017 for use in a translation model.1 \n",
            "It’s a sequence-to-sequence model capable of converting sequences from one domain \n",
            "into sequences in another domain. For example, translating French sentences to English \n",
            "sentences. The original transformer architecture consists of two parts: an encoder and a \n",
            "decoder. The encoder converts the input text (e.g., a French sentence) into a representation, \n",
            "which is then passed to the decoder. The decoder uses this representation to generate the \n",
            "output text (e.g., an English translation) autoregressively.1 Notably, the size of the output of \n",
            "the transformer encoder is linear in the size of its input. Figure 1 shows the design of the \n",
            "original transformer architecture.\n",
            "The transformer consists of multiple layers. A layer in a neural network comprises a set of \n",
            "parameters that perform a specific transformation on the data. In the diagram you can see \n",
            "an example of some layers which include Multi-Head Attention, Add & Norm, Feed-Forward, \n",
            "Linear, Softmax etc. The layers can be sub-divided into the input, hidden and output layers. \n",
            "The input layer (e.g., Input/Output Embedding) is the layer where the raw data enters the \n",
            "network. Input embeddings  are used to represent the input tokens to the model. Output \n",
            "embeddings  are used to represent the output tokens that the model predicts. For example, in \n",
            "a machine translation model, the input embeddings would represent the words in the source \n",
            "language, while the output embeddings would represent the words in the target language. \n",
            "The output layer (e.g., Softmax) is the final layer that produces the output of the network. The \n",
            "hidden layers (e.g., Multi-Head Attention) are between the input and output layers and are \n",
            "where the magic happens!\n",
            "Foundational Large Language Models & Text Generation10\n",
            "September 2024\n",
            "Figure 1. Original Transformer1 (P.C:5)\n",
            "Foundational Large Language Models & Text Generation11\n",
            "September 2024To better understand the different layers in the transformer, let’s use a French-to-English \n",
            "translation task as an example. Here, we explain how a French sentence is input into the \n",
            "transformer and a corresponding English translation is output. We will also describe each of \n",
            "the components inside the transformer from Figure 1.\n",
            "Input preparation and embedding\n",
            "To prepare language inputs for transformers, we convert an input sequence into tokens and \n",
            "then into input embeddings. At a high level, an input embedding is a high-dimensional vector \n",
            "that represents the meaning of each token in the sentence. This embedding is then fed into \n",
            "the transformer for processing. Generating an input embedding involves the following steps:\n",
            "1. Normalization  (Optional): Standardizes text by removing redundant whitespace, \n",
            "accents, etc.\n",
            "2. Tokenization : Breaks the sentence into words or subwords and maps them to integer \n",
            "token IDs from a vocabulary.\n",
            "3. Embedding : Converts each token ID to its corresponding high-dimensional vector, \n",
            "typically using a lookup table. These can be learned during the training process.\n",
            "4. Positional  Encoding : Adds information about the position of each token in the sequence \n",
            "to help the transformer understand word order.\n",
            "These steps help to prepare the input for the transformers so that they can better \n",
            "understand the meaning of the text.\n",
            "Foundational Large Language Models & Text Generation12\n",
            "September 2024Multi-head attention\n",
            "After converting input tokens into embedding vectors, you feed these embeddings into \n",
            "the multi-head attention module (see Figure 1). Self-attention is a crucial mechanism in \n",
            "transformers; it enables them to focus on specific parts of the input sequence relevant to \n",
            "the task at hand and to capture long-range dependencies within sequences more effectively \n",
            "than traditional RNNs. \n",
            "Understanding self-attention\n",
            "Consider the following sentence: “The tiger jumped out of a tree to get a drink because it \n",
            "was thirsty.” Self-attention helps to determine relationships between different words and \n",
            "phrases in sentences. For example, in this sentence, “the tiger” and “it” are the same object, \n",
            "so we would expect these two words to be strongly connected. Self-attention achieves this \n",
            "through the following steps (Figure 2):\n",
            "1. Creating queries, keys, and values:  Each input embedding is multiplied by three learned \n",
            "weight matrices (Wq, Wk, Wv) to generate query (Q), key (K), and value (V) vectors. These \n",
            "are like specialized representations of each word.\n",
            "• Query: The query vector helps the model ask, “Which other words in the sequence are \n",
            "relevant to me?”\n",
            "• Key: The key vector is like a label that helps the model identify how a word might be \n",
            "relevant to other words in the sequence.\n",
            "• Value: The value vector holds the actual word content information.\n",
            "2. Calculating scores:  Scores are calculated to determine how much each word should \n",
            "‘attend’ to other words. This is done by taking the dot product of the query vector of one \n",
            "word with the key vectors of all the words in the sequence.\n",
            "Foundational Large Language Models & Text Generation13\n",
            "September 20243. Normalization:  The scores are divided by the square root of the key vector dimension (dk) \n",
            "for stability, then passed through a softmax function to obtain attention weights. These \n",
            "weights indicate how strongly each word is connected to the others.\n",
            "4. Weighted values:  Each value vector is multiplied by its corresponding attention weight. \n",
            "The results are summed up, producing a context-aware representation for each word.\n",
            "Figure 2. The process of computing self-attention in the multi-head attention module1 (P.C:5)  \n",
            "Foundational Large Language Models & Text Generation14\n",
            "September 2024In practice, these computations are performed at the same time, by stacking the query, key \n",
            "and value vectors for all the tokens into Q, K and V matrices and multiplying them together as \n",
            "shown in Figure 3.\n",
            "Figure 3. The basic operation of attention,1  with Q=query, K=Keys and V=Value, Z=Attention, d_k = dimension \n",
            "of queries and keys (P.C:5)\n",
            "Multi-head attention: power in diversity\n",
            "Multi-head attention employs multiple sets of Q, K, V weight matrices. These run in parallel, \n",
            "each ‘head’ potentially focusing on different aspects of the input relationships. The outputs \n",
            "from each head are concatenated and linearly transformed, giving the model a richer \n",
            "representation of the input sequence.\n",
            "The use of multi-head attention improves the model’s ability to handle complex language \n",
            "patterns and long-range dependencies. This is crucial for tasks that require a nuanced \n",
            "understanding of language structure and content, such as machine translation, text \n",
            "summarization, and question-answering. The mechanism enables the transformer to consider \n",
            "multiple interpretations and representations of the input, which enhances its performance on \n",
            "these tasks. \n",
            "Foundational Large Language Models & Text Generation15\n",
            "September 2024Layer normalization and residual connections\n",
            "Each layer in a transformer, consisting of a multi-head attention module and a feed-forward \n",
            "layer, employs layer normalization and residual connections. This corresponds to the Add \n",
            "and Norm layer in Figure 1, where ‘Add’ corresponds to the residual connection and ‘Norm’ \n",
            "corresponds to layer normalization. Layer normalization computes the mean and variance \n",
            "of the activations to normalize the activations in a given layer. This is typically performed to \n",
            "reduce covariate shift as well as improve gradient flow to yield faster convergence during \n",
            "training as well as improved overall performance. \n",
            "Residual connections propagate the inputs to the output of one or more layers. This has the \n",
            "effect of making the optimization procedure easier to learn and also helps deal with vanishing \n",
            "and exploding gradients. \n",
            "The Add and Norm  layer is applied to both the multi-head attention module and the feed-\n",
            "forward layer described in the following section.\n",
            "Feedforward layer \n",
            "The output of the multi-head attention module and the subsequent ‘Add and Norm’ layer is \n",
            "fed into the feedforward layer of each transformer block. This layer applies a position-wise \n",
            "transformation to the data, independently for each position in the sequence, which allows the \n",
            "incorporation of additional non-linearity and complexity into the model’s representations. The \n",
            "feedforward layer typically consists of two linear transformations with a non-linear activation \n",
            "function, such as ReLU or GELU, in between. This structure adds further representational \n",
            "power to the model. After processing by the feedforward layer, the data undergoes \n",
            "another ‘Add and Norm’ step, which contributes to the stability and effectiveness of deep \n",
            "transformer models.\n",
            "Foundational Large Language Models & Text Generation16\n",
            "September 2024Encoder and decoder\n",
            "The original transformer architecture relies on a combination of encoder and decoder \n",
            "modules. Each encoder and decoder consists of a series of layers, with each layer \n",
            "comprising key components: a multi-head self-attention mechanism, a position-wise feed-\n",
            "forward network, normalization layers, and residual connections. \n",
            "The encoder’s primary function is to process the input sequence into a continuous \n",
            "representation that holds contextual information for each token. The input sequence is first \n",
            "normalized, tokenized, and converted into embeddings. Positional encodings are added to \n",
            "these embeddings to retain sequence order information. Through self-attention mechanisms, \n",
            "each token in the sequence can dynamically attend to any other token, thus understanding \n",
            "the contextual relationships within the sequence. The output from the encoder is a series of \n",
            "embedding vectors Z representing the entire input sequence. \n",
            "The decoder is tasked with generating an output sequence based on the context provided \n",
            "by the encoder’s output Z. It operates in a token-by-token fashion, beginning with a start-\n",
            "of-sequence token. The decoder layers employ two types of attention mechanisms: masked \n",
            "self-attention  and encoder-decoder cross-attention. Masked self-attention ensures that \n",
            "each position can only attend to earlier positions in the output sequence, preserving the \n",
            "auto-regressive property. This is crucial for preventing the decoder from having access to \n",
            "future tokens in the output sequence. The encoder-decoder cross-attention mechanism \n",
            "allows the decoder to focus on relevant parts of the input sequence, utilizing the contextual \n",
            "embeddings generated by the encoder. This iterative process continues until the decoder \n",
            "predicts an end-of-sequence token, thereby completing the output sequence generation.\n",
            "Majority of recent LLMs adopted a decoder-only  variant of transformer architecture. This \n",
            "approach forgoes the traditional encoder-decoder separation, focusing instead on directly \n",
            "generating the output sequence from the input. The input sequence undergoes a similar \n",
            "Foundational Large Language Models & Text Generation17\n",
            "September 2024process of embedding and positional encoding before being fed into the decoder. The \n",
            "decoder then uses masked self-attention to generate predictions for each subsequent \n",
            "token based on the previously generated tokens. This streamlined approach simplifies the \n",
            "architecture for specific tasks where encoding and decoding can be effectively merged.\n",
            "Training the transformer\n",
            "When talking about machine learning models, it’s important to differentiate between \n",
            "training and inference. Training typically refers to modifying the parameters of the model, \n",
            "and involves loss functions and backpropagation. Inference is when model is used only \n",
            "for the predicted output, without updating the model weights. The model parameters are \n",
            "fixed during inference. Up until now we learned how transformers generate outputs during \n",
            "inference. Next, we focus on how to train transformers to perform one or more given tasks.\n",
            "Data preparation\n",
            "The first step is data preparation, which involves a few important steps itself. First, clean the \n",
            "data by applying techniques such as filtering, deduplication, and normalization. The next \n",
            "step is tokenization where the dataset is converted into tokens using techniques such as \n",
            "Byte-Pair Encoding8, 9 and Unigram tokenization.8, 10 Tokenization generates a vocabulary, \n",
            "which is a set of unique tokens used by the LLM. This vocabulary serves as the model’s \n",
            "’language’ for processing and understanding text. Finally, the data is typically split into a \n",
            "training dataset for training the model as well as a test dataset which is used to evaluate the \n",
            "models performance.\n",
            "Foundational Large Language Models & Text Generation18\n",
            "September 2024Training and loss function\n",
            "A typical transformer training loop consists of several parts: First, batches of input \n",
            "sequences are sampled from a training dataset. For each input sequence, there is a \n",
            "corresponding target sequence. In unsupervised pre-training, the target sequence is \n",
            "derived from the input sequence itself. The batch of input sequences is then fed into the \n",
            "transformer. The transformer generates predicted output sequences. The difference \n",
            "between the predicted and target sequences is measured using a loss function (often cross-\n",
            "entropy loss)11. Gradients of this loss are calculated, and an optimizer uses them to update \n",
            "the transformer’s parameters. This process is repeated until the transformer converges to a \n",
            "certain level of performance or until it has been trained on a pre-specified number of tokens. \n",
            "There are different approaches to formulating the training task for transformers depending \n",
            "on the architecture used:\n",
            "• Decoder-only  models are typically pre-trained on the language modeling task (e.g., see \n",
            "endnote12, 13). The target sequence for the decoder is simply a shifted version of the input \n",
            "sequence. Given a training sequence like ‘the cat sat on the mat’ various input/target \n",
            "pairs can be generated for the model. For example the input “the cat sat on” should \n",
            "predict “the” and subsequently the input “the cat sat on the” should predict target \n",
            "sequence “mat”.\n",
            "• Encoder-only  models (like BERT)14 are often pre-trained by corrupting the input sequence \n",
            "in some way and having the model try to reconstruct it. One such approach is masked \n",
            "language modeling (MLM).14 In our example, the input sequence could be “The [MASK] sat \n",
            "on the mat” and the sequence target would be the original sentence.\n",
            "• Encoder-decoder  models (like the original transformer) are trained on sequence-to-\n",
            "sequence supervised tasks such as translation (input sequence “Le chat est assis sur \n",
            "le tapis” and target “The cat sat on the mat”), question-answering (where the input \n",
            "sequence is a question and the target sequence is the corresponding answer), and \n",
            "Foundational Large Language Models & Text Generation19\n",
            "September 2024summarization (where the input sequence is a full article and the target sequence is its \n",
            "corresponding summary). These models could also be trained in an unsupervised way by \n",
            "converting other tasks into sequence-to-sequence format. For example, when training \n",
            "on Wikipedia data, the input sequence might be the first part of an article, and the target \n",
            "sequence comprises the remainder of the article.\n",
            "An additional factor to consider during training is the ‘context length’. This refers to the \n",
            "number of previous tokens the model can ‘remember’ and use to predict the next token in \n",
            "the sequence. Longer context lengths allow the model to capture more complex relationships \n",
            "and dependencies within the text, potentially leading to better performance. However, longer \n",
            "contexts also require more computational resources and memory, which can slow down \n",
            "training and inference. Choosing an appropriate context length involves balancing these \n",
            "trade-offs based on the specific task and available resources.\n",
            "The evolution of transformers\n",
            "The next sections provide an overview of the various transformer architectures. These \n",
            "include encoder-only, encoder-decoder, as well as decoder-only transformers. We start with \n",
            "GPT-1 and BERT and end with Google’s latest family of LLMs called Gemini.\n",
            "GPT-1\n",
            "GPT-1 (Generative pre-trained transformer version 1)15 was a decoder-only  model developed \n",
            "by OpenAI in 2018. It was trained on the BooksCorpus dataset (containing approximately \n",
            "several billion words) and is able to generate text, translate languages, write different kinds \n",
            "of creative content, and answer questions in an informative way. The main innovations in \n",
            "GPT-1 were:\n",
            "Foundational Large Language Models & Text Generation20\n",
            "September 2024• Combining transformers and unsupervised pre-training: Unsupervised pre-training \n",
            "is a process of training a language model on a large corpus of unlabeled data. Then, \n",
            "supervised data is used to fine-tune the model for a specific task, such as translation \n",
            "or sentiment classification. In prior works, most language models were trained using a \n",
            "supervised learning objective. This means that the model was trained on a dataset of \n",
            "labeled data, where each example had a corresponding label. This approach has two main \n",
            "limitations. First, it requires a large amount of labeled data, which can be expensive and \n",
            "time-consuming to collect. Second, the model can only generalize to tasks that are similar \n",
            "to the tasks that it was trained on. Semi-supervised sequence learning was one of the first \n",
            "works that showed that unsupervised pre-training followed by supervised training was \n",
            "superior than supervised training alone.\n",
            "Unsupervised pre-training addresses these limitations by training the model on a large \n",
            "corpus of unlabeled data. This data can be collected more easily and cheaply than labeled \n",
            "data. Additionally, the model can generalize to tasks that are different from the tasks that \n",
            "it was trained on. The BooksCorpus dataset is a large (5GB) corpus of unlabeled text that \n",
            "was used to train the GPT-1 language model. The dataset contains over 7,000 unpublished \n",
            "books, which provides the model with a large amount of data to learn from. Additionally, \n",
            "the corpus contains long stretches of contiguous text, which helps the model learn long-\n",
            "range dependencies. Overall, unsupervised pre-training is a powerful technique that can \n",
            "be used to train language models that are more accurate and generalizable than models \n",
            "that are trained using supervised learning alone. \n",
            "• Task-aware input transformations: There are different kinds of tasks such as textual \n",
            "entailment and question-answering that require a specific structure. For example, \n",
            "textual entailment requires a premise and a hypothesis; question-answering requires a \n",
            "context document; a question  and possible answers . One of the contributions of GPT-1 \n",
            "is converting these types of tasks which require structured inputs into an input that the \n",
            "language model can parse, without requiring task-specific architectures on top of the \n",
            "pre-trained architecture. For textual entailment, the premise p and the hypothesis h are \n",
            "Foundational Large Language Models & Text Generation21\n",
            "September 2024concatenated with a delimiter token ($) in between - [ p, $, h]. For question answering, the \n",
            "context document c is concatenated with the question q and a possible answer a with a \n",
            "delimiter token in between the question and answer - [ c,q,$,a].\n",
            "GPT-1 surpassed previous models on several benchmarks, achieving excellent results. While \n",
            "GPT-1 was a significant breakthrough in natural language processing (NLP), it had some \n",
            "limitations. For example, the model was prone to generating repetitive text, especially when \n",
            "given prompts outside the scope of its training data. It also failed to reason over multiple \n",
            "turns of dialogue and could not track long-term dependencies in text. Additionally, its \n",
            "cohesion and fluency were limited to shorter text sequences, and longer passages would \n",
            "lack cohesion. Despite these limitations, GPT-1 demonstrated the power of unsupervised \n",
            "pre-training, which laid the foundation for larger and more powerful models based on the \n",
            "transformer architecture.\n",
            "BERT\n",
            "BERT14 which stands for Bidirectional Encoder Representations from Transformers, \n",
            "distinguishes itself from traditional encoder-decoder transformer models by being an \n",
            "encoder-only architecture. Instead of translating or producing sequences, BERT focuses \n",
            "on understanding context deeply by training on a masked language model objective. In \n",
            "this setup, random words in a sentence are replaced with a [MASK] token, and BERT tries \n",
            "to predict the original word based on the surrounding context. Another innovative aspect \n",
            "of BERT’s training regime is the next sentence prediction loss, where it learns to determine \n",
            "whether a given sentence logically follows a preceding one. By training on these objectives, \n",
            "BERT captures intricate context dependencies from both the left and right of a word, and \n",
            "it can discern the relationship between pairs of sentences. Such capabilities make BERT \n",
            "especially good at tasks that require natural language understanding, such as question-\n",
            "answering, sentiment analysis, and natural language inference, among others. Since this is an \n",
            "encoder-only model, BERT cannot generate text.\n",
            "Foundational Large Language Models & Text Generation22\n",
            "September 2024GPT-2\n",
            "GPT-2,12 the successor to GPT-1, was released in 2019 by OpenAI. The main innovation of \n",
            "GPT-2 was a direct scale-up, with a tenfold increase in both its parameter count and the size \n",
            "of its training dataset:\n",
            "• Data: GPT-2 was trained on a large (40GB) and diverse dataset called WebText, which \n",
            "consists of 45 million webpages from Reddit with a Karma rating of at least three. Karma \n",
            "is a rating metric used on Reddit and a value of three means that all the posts were of a \n",
            "reasonable level of quality.\n",
            "• Parameters:  GPT-2 had 1.5 billion parameters, which was an order of magnitude larger \n",
            "than the previous model. More parameters increase the model’s learning capacity. The \n",
            "authors trained four language models with 117M (the same as GPT-1), 345M, 762M, and 1.5B \n",
            "(GPT-2) parameters, and found that the model with the most parameters performed better \n",
            "on every subsequent task.\n",
            "This scaling up resulted in a model that was able to generate more coherent and realistic text \n",
            "than GPT-1. Its ability to generate human-like responses made it a valuable tool for various \n",
            "natural language processing tasks, such as content creation and translation. Specifically, \n",
            "GPT-2 demonstrated significant improvement in capturing long-range dependencies and \n",
            "common sense reasoning. While it performed well in some tasks, it did not outperform state-\n",
            "of-the-art reading comprehension, summarization, and translation. GPT-2’s most significant \n",
            "achievement was its ability to perform zero-shot learning on a variety of tasks. Zero-shot task \n",
            "transfer is the ability of a model to generalize to a new task without being trained on it, which \n",
            "requires the model to understand the task based on the given instruction. For example, for \n",
            "an English to German translation task, the model might be given an English sentence followed \n",
            "by the word “German” and a prompt (“:”). The model would then be expected to understand \n",
            "that this is a translation task and generate the German translation of the English sentence. \n",
            "GPT-2 was able to perform tasks such as machine translation, text summarization, and \n",
            "reading comprehension without any explicit supervision.\n",
            "Foundational Large Language Models & Text Generation23\n",
            "September 2024The study discovered that performance on zero-shot tasks increased in a log-linear manner \n",
            "as the model’s capacity increased. GPT-2 showed that training on a larger dataset and having \n",
            "more parameters improved the model’s ability to understand tasks and surpass the state-of-\n",
            "the-art on many tasks in zero-shot settings.\n",
            "GPT-3/3.5/4\n",
            "GPT-3,13 or the third iteration of the Generative Pre-trained Transformer model, represents a \n",
            "significant evolution from its predecessor, GPT-2, primarily in terms of scale, capabilities, and \n",
            "flexibility. The most noticeable difference is the sheer size of GPT-3, boasting a whopping \n",
            "175 billion parameters, compared to GPT-2’s largest model which had 1.5 billion parameters. \n",
            "This increase in model size allowed GPT-3 to store and recall an even more vast amount of \n",
            "information, understand nuanced instructions, and generate more coherent and contextually \n",
            "relevant text over longer passages.\n",
            "While GPT-2 could be fine-tuned on specific tasks with additional training data, GPT-3 can \n",
            "understand and execute tasks with just a few examples, or sometimes even without any \n",
            "explicit examples—simply based on the instruction provided. This highlights GPT-3’s more \n",
            "dynamic understanding and adaptation abilities, reducing the need for task-specific fine-\n",
            "tuning which was more prevalent in GPT-2.\n",
            "Finally, GPT-3’s large model scale and diverse training corpus have led to better \n",
            "generalization across a broader range of tasks. This means that out-of-the-box, without \n",
            "any further training, GPT-3 exhibits improved performance on diverse NLP challenges, from \n",
            "translation to question-answering, compared to GPT-2. It’s also worth noting that the release \n",
            "approach differed: while OpenAI initially held back GPT-2 due to concerns about misuse, \n",
            "they chose to make GPT-3 available as a commercial API, reflecting both its utility and the \n",
            "organization’s evolving stance on deployment.\n",
            "Foundational Large Language Models & Text Generation24\n",
            "September 2024Instruction tuning was then introduced with InstructGPT17, a version of GPT-3 that was fine-\n",
            "tuned, using Supervised Fine-Tuning, on a dataset of human demonstrations of desired \n",
            "model behaviors. Outputs from this model were then ranked and it was then further fine-\n",
            "tuned using Reinforcement Learning from Human Feedback. This led to improved instruction \n",
            "following in the model. A 1.3B parameter InstructGPT model had better human evaluations \n",
            "than the 175B parameter GPT-3 model. It also showed improvements in truthfulness and \n",
            "reductions in toxicity.\n",
            "GPT-3.5 models, including GPT-3.5 turbo, improve over GPT-3 as it is capable of \n",
            "understanding and generating code. It’s been optimized for dialogue. And it’s capable of \n",
            "receiving context windows of up to 16,385 tokens and can generate outputs of up to 4,096 \n",
            "tokens. \n",
            "GPT-4 extends GPT-3.5 as a large multimodal model capable of processing image and \n",
            "text inputs and producing text outputs.19 Specifically, accepting text or images as input \n",
            "and outputting text. This model has broader general knowledge and advanced reasoning \n",
            "capabilities. It can receive context windows of up to 128,000 tokens and has a maximum \n",
            "output of 4,096 tokens. GPT-4 demonstrates remarkable versatility by solving complex tasks \n",
            "across diverse fields like mathematics, coding, vision, medicine, law, and psychology – all \n",
            "without specialized instructions. Its performance often matches or even exceeds human \n",
            "capabilities and significantly outperforms earlier models like GPT-3.5.\n",
            "LaMDA\n",
            "Google’s LaMDA,20 which stands for ‘Language Model for Dialogue Applications’ is another \n",
            "contribution to the arena of large-scale language models, designed primarily to engage in \n",
            "open-ended conversations. Unlike traditional chatbots which operate in more constrained \n",
            "and predefined domains, LaMDA is engineered to handle a wide array of topics, delivering \n",
            "Foundational Large Language Models & Text Generation25\n",
            "September 2024more natural and flowing conversations. LaMDA was trained on dialogue-focused data to \n",
            "encourage ongoing conversational flow, not just isolated responses, ensuring users can have \n",
            "more extensive and explorative dialogues.\n",
            "While GPT models, especially the later iterations like GPT-3, have strived to address a \n",
            "multitude of tasks simultaneously, from text generation to code writing, LaMDA’s primary \n",
            "focus is on maintaining and enhancing conversational depth and breadth. GPT models \n",
            "shine on their ability to produce coherent long-form content and perform various tasks \n",
            "with minimal prompting, whereas LaMDA emphasizes the flow and progression of dialogue, \n",
            "striving to mimic the unpredictability and richness of human conversations. \n",
            "Gopher\n",
            "Gopher22 is a 280 billion parameter language model based on the decoder-only transformer \n",
            "architecture, developed by DeepMind in 2021.22 It can generate text, translate languages, \n",
            "write different kinds of creative content, and answer your questions in an informative way. \n",
            "Similar to GPT-3, Gopher focused on improving dataset quality and optimization techniques:\n",
            "• Dataset: The researchers curated a high-quality text dataset called MassiveText, which \n",
            "contains over 10 terabytes of data and 2.45B documents from web pages, books, news \n",
            "articles, and code (GitHub). They only trained on 300B tokens, which is 12% of the dataset. \n",
            "Importantly, they improved the quality of the data by filtering it, such as by removing \n",
            "duplicate text and deduplicating similar documents. This significantly improved the \n",
            "model’s performance on downstream tasks.\n",
            "• Optimization:  The researchers used a warmup learning rate for 1,500 steps and then \n",
            "decayed it using a cosine schedule. They also had an interesting rule that as they \n",
            "increased the model size, they decreased the learning rate and increased the number of \n",
            "tokens in each batch. Additionally, they found that clipping gradients to be a maximum of 1 \n",
            "based on the global gradient norm helped stabilize the training.\n",
            "Foundational Large Language Models & Text Generation26\n",
            "September 2024Gopher was evaluated on a variety of tasks, including mathematics, common sense, logical \n",
            "reasoning, general knowledge, scientific understanding, ethics, and reading comprehension. \n",
            "Gopher outperformed previous state-of-the-art models on 81% of the tasks. Specifically, \n",
            "Gopher performed well on knowledge-intensive tasks but struggled on reasoning-heavy \n",
            "tasks such as abstract algebra.\n",
            "The authors also conducted a study on the effect of model size on different types of \n",
            "tasks. Figure 4 shows the results of this ablation study. Specifically, the authors found that \n",
            "increasing the number of parameters had a significant impact on logical reasoning and \n",
            "reading comprehension, but it did not improve performance as much on tasks such as \n",
            "general knowledge, where performance eventually almost plateaued.\n",
            "Figure 4. Ablation study22 on the effect of model size on the performance of Gopher on different types \n",
            "of tasks\n",
            "GLaM\n",
            "GLaM (Generalist Language Model)23 was the first sparsely-activated mixture-of-experts \n",
            "language model. Mixture-of-experts based models are much more computationally efficient \n",
            "given their parameter count. This is achieved by only activating a subset of their parameters \n",
            "Foundational Large Language Models & Text Generation27\n",
            "September 2024(i.e. experts) for each input token. GLaM consists of 1.2 trillion parameters but uses only ⅓ \n",
            "of the energy used to train GPT-3 and half of the FLOPs for inference while achieving better \n",
            "overall performance compared to GPT-3.\n",
            "Chinchilla\n",
            "Until 2022, LLMs were primarily scaled by increasing the model size and using datasets that \n",
            "are relatively small by current standards (up to 300 billion tokens for the largest models). \n",
            "This approach was informed by the Kaplan et al.24 study, which examined how performance \n",
            "of a language model, measured by cross-entropy loss, varies with changes in computational \n",
            "budget, model size, and dataset size. Specifically, given a 100-fold increase in computational \n",
            "resources ( C), Kaplan et al.24 recommended scaling model size by approximately 28.8 times \n",
            "(Nopt∝ C0.73), while increasing dataset size by only 3.5 times ( Dopt∝ C0.27). \n",
            "The Chinchilla paper,25 revisited the compute optimal scaling laws and used three different \n",
            "approaches to find that near equal scaling in parameters and data is optimal with increasing \n",
            "compute. Thus, a 100-fold increase in compute should translate into a tenfold increase in \n",
            "both data size and model size. \n",
            "Figure 5. Overlaid predictions from three different approaches from Chinchilla paper,25 along with \n",
            "projections from Kaplan et al24 \n",
            "Foundational Large Language Models & Text Generation28\n",
            "September 2024To verify the updated scaling law, DeepMind trained a 70B parameter model (called \n",
            "Chinchilla) using the same compute budget as the previously trained Gopher model. \n",
            "Chinchilla uniformly and significantly outperformed Gopher (280B),21 GPT-3 (175B),13 and \n",
            "Megatron-Turing NLG (530B)26 on a large range of downstream evaluation tasks. Due to being \n",
            "4x smaller than Gopher, both the memory footprint and the inference cost of Chinchilla are \n",
            "also smaller.\n",
            "The findings of Chinchilla had significant ramifications for the development of future LLMs. \n",
            "Focus shifted into finding ways to scale dataset size (while maintaining quality) alongside \n",
            "increasing parameter count. Extrapolating this trend suggests that training dataset size \n",
            "may soon be limited by the amount of text data available. This has led to new research by \n",
            "Muennighoff et al.27 exploring scaling laws in data-constrained regimes.\n",
            "PaLM\n",
            "Pathways  language model (PaLM)28 is a 540-billion parameter transformer-based large \n",
            "language model developed by Google AI. It was trained on a massive dataset of text and \n",
            "code and is capable of performing a wide range of tasks, including common sense reasoning, \n",
            "arithmetic reasoning, joke explanation, code generation, and translation.\n",
            "At the time of its release, PaLM was also able to achieve state-of-the-art performance on \n",
            "many language benchmarks, for example GLUE and SuperGLUE.29\n",
            "One of the key features of PaLM is its ability to scale efficiently. This is thanks to the \n",
            "Pathways system, which Google developed to distribute the training of large language \n",
            "models across two TPU v4 Pods.\n",
            "Foundational Large Language Models & Text Generation29\n",
            "September 2024PaLM 2\n",
            "PaLM 230 is a successor to PaLM that was announced in May 2023. Thanks to a number of \n",
            "architectural and training enhancements, PaLM 2 is even more capable than PaLM, with \n",
            "fewer total parameters. It excels at advanced reasoning tasks, including code generation, \n",
            "math, classification, question answering, and translation.\n",
            "PaLM 2 has also been shown to be more efficient than PaLM and became the basis for a \n",
            "number of commercial models Google released as part of Google Cloud Generative AI.\n",
            "Gemini\n",
            "Figure 6. Gemini can receive multi-modal inputs including text, audio, images, and video data. These are all \n",
            "tokenized and fed into its transformer model. The transformer generates an output that can contain images \n",
            "and text \n",
            "Foundational Large Language Models & Text Generation30\n",
            "September 2024Gemini31 (Figure 6) is a state-of-the-art multimodal language family of models that can \n",
            "take interleaved sequences of text, image, audio, and video as input. It’s built on top of \n",
            "transformer decoders and has architectural improvements for scale as well as optimized \n",
            "inference on Google’s Tensor Processing Units (TPUs). In its current 1.5 version, these models \n",
            "are trained to support contexts of different sizes, up to 2M tokens in the Gemini 1.5 Pro \n",
            "version on Vertex AI and employ mechanisms such as multi-query attention for efficiency. \n",
            "Gemini models also employ a Mixture of Experts architecture to optimize efficiency and \n",
            "capabilities of the models. Multimodality allows the models to process text, images and video \n",
            "in input, with more modalities in input and output expected in the future.\n",
            "The Gemini models are trained on Google’s TPUv5e and TPUv4 processors, depending on \n",
            "size and configuration. The pre-training data consists of web documents, books, code, and \n",
            "image, audio, and video data. \n",
            "Larger models are trained for the compute-optimal number of tokens using the same \n",
            "approach as in Chinchilla paper,25 while small models are trained on significantly more tokens \n",
            "than compute optimal to improve performance for a given inference budget.\n",
            "The Gemini family of models is optimized for different sizes: Gemini Ultra, Gemini Pro, Gemini \n",
            "Nano and Flash. Gemini Ultra is used for highly complex tasks and achieves state-of-the-\n",
            "art results in 30 out of 32 benchmark tasks. Gemini Pro enables deployment at scale and \n",
            "Gemini Nano is designed for on-device applications. The Gemini Nano models leverage \n",
            "advancements such as distillation to produce state-of-the-art performance for small \n",
            "language models on tasks such as summarization and reading comprehension. As the Gemini \n",
            "models are natively multi-modal, it can be seen that training across multiple modalities does \n",
            "indeed lead to a model that is capable of achieving strong capabilities in each domain. \n",
            "Foundational Large Language Models & Text Generation31\n",
            "September 2024During the initial part of 2024, Google introduced the latest model of the Gemini family, \n",
            "Gemini 1.5 Pro,32 a highly compute-efficient multimodal mixture-of-experts model. This \n",
            "model  also dramatically increased the size of the context window to millions of tokens \n",
            "and is capable of recalling and reasoning over those millions of tokens, including multiple \n",
            "long documents and hours of video and audio. Gemini 1.5 Pro demonstrates remarkable \n",
            "capabilities across different domains:\n",
            "• Code understanding: It can process massive codebases and answer highly specific \n",
            "code-related questions.\n",
            "• Language learning: The model can learn new languages never observed at training time \n",
            "solely based on reference materials provided within its input\n",
            "• Multimodal reasoning: It understands images and text, allowing it to locate a famous scene \n",
            "from the novel ‘Les Misérables’ based on a simple sketch.\n",
            "• Video comprehension: It can analyze entire movies, answering detailed questions and \n",
            "pinpointing specific timestamps with remarkable accuracy.\n",
            "Google’s Gemini 1.5 Pro model excels at retrieving information from even very long \n",
            "documents. In their study,32 it demonstrated 100% recall on documents up to 530,000 \n",
            "tokens, and over 99.7% recall on those up to 1 million tokens. Impressively, it maintains 99.2% \n",
            "accuracy when finding information in documents up to 10 million tokens.\n",
            "Moreover, Gemini 1.5 Pro demonstrates a major leap forward in how well LLMs follow complex \n",
            "instructions. In a rigorous test with 406 multi-step prompts, it significantly outperformed \n",
            "previous Gemini models. The model accurately followed almost 90% of instructions and fully \n",
            "completed 66% of the complex tasks. \n",
            "Foundational Large Language Models & Text Generation32\n",
            "September 2024Gemini Flash is a new addition to the Gemini model family and the fastest Gemini model \n",
            "served in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more \n",
            "cost-efficient to serve and features a breakthrough long context window of 1 million tokens. \n",
            "Although it is a lighter weight model than 1.5 Pro, it is highly capable of multimodal reasoning \n",
            "across vast amounts of information and delivers impressive quality for its size.\n",
            "Furthermore, recently advanced Gemma is a family of lightweight, state-of-the-art open \n",
            "models built from the same research and technology used to create the Gemini models.33 The \n",
            "first model by Gemma boasts a large vocabulary of 256,000 words and has been trained on \n",
            "a massive 6 trillion token dataset. This makes it a valuable addition to the openly-available \n",
            "LLM collection. Additionally, the 2B parameter version is intriguing as it can run efficiently on \n",
            "a single GPU.\n",
            "Gemma 2,33 developed by Google AI, represents a significant advancement in the field of \n",
            "open large language models. Designed with a focus on efficiency, the 27-billion parameter \n",
            "model boasts performance comparable to much larger models like Llama 3 70B33 on standard \n",
            "benchmarks. This makes Gemma 2 a powerful and accessible tool for a wide range of AI \n",
            "developers. Its compatibility with diverse tuning toolchains, from cloud-based solutions \n",
            "to popular community tools, further enhances its versatility. With its strong performance, \n",
            "efficient architecture, and accessible nature, Gemma 2 plays a vital role in driving innovation \n",
            "and democratizing AI capabilities.\n",
            "Other open models\n",
            "The landscape of open LLMs is rapidly evolving, with a growing number of models where \n",
            "both the code and pre-trained weights are publicly accessible. Below we highlight some of \n",
            "the known examples:\n",
            "Foundational Large Language Models & Text Generation33\n",
            "September 2024• LLaMA 234: Released by Meta AI, LLaMA 2 is a family of pretrained and fine-tuned \n",
            "LLMs ranging from 7B to 70B parameters. It shows significant improvements over its \n",
            "predecessor, LLaMA 1, including a 40% larger pre-training dataset (2 trillion tokens), \n",
            "doubled context length (4096 tokens), and the use of grouped-query attention. The \n",
            "fine-tuned version, LLaMA 2-Chat, is optimized for dialogue and shows competitive \n",
            "performance against closed-source models of the same size.\n",
            "• LLaMA 3.221: Released by Meta AI, LLaMA 3.2 is the next generation of their open LLMs. \n",
            "Llama 3.2 includes multilingual text-only models (1B, 3B) and vision LLMs (11B, 90B), with \n",
            "quantized versions of 1B and 3B offering on average up to 56% smaller size and 2-3x \n",
            "speedup, ideal for on-device and edge deployments. LLaMA 3.2 utilizes grouped-query \n",
            "attention and a 128K token vocabulary for enhanced performance and efficiency.\n",
            "• Mixtral35: Developed by Mistral AI, Mixtral 8x7B is a Sparse Mixture of Experts (SMoE) \n",
            "model. While its total parameter count is 47B, it utilizes only 13B active parameters per \n",
            "token during inference, leading to faster inference and higher throughput. This model \n",
            "excels in mathematics, code generation, and multilingual tasks, often outperforming \n",
            "LLaMA 2 70B in these domains. Mixtral also supports a 32k token context length, enabling \n",
            "it to handle significantly longer sequences. Its instruction-tuned version, Mixtral 8x7B-\n",
            "Instruct, surpasses several closed-source models on human evaluation benchmarks.\n",
            "• Qwen 1.536: This LLM series from Alibaba comes in six sizes: 0.5B, 1.8B, 4B, 7B, 14B, and \n",
            "72B. Qwen 1.5 models uniformly support a context length of up to 32k tokens and show \n",
            "strong performance across various benchmarks. Notably, Qwen 1.5-72B outperforms \n",
            "LLaMA2-70B on all evaluated benchmarks, demonstrating exceptional capabilities in \n",
            "language understanding, reasoning, and math.\n",
            "• Yi37: Created by 01.AI, the Yi model family includes 6B and 34B base models pre-trained \n",
            "on a massive 3.1 trillion token English and Chinese dataset. Yi emphasizes data quality \n",
            "through rigorous cleaning and filtering processes. The 34B model achieves performance \n",
            "Foundational Large Language Models & Text Generation34\n",
            "September 2024comparable to GPT-3.5 on many benchmarks and can be efficiently served on consumer-\n",
            "grade GPUs with 4-bit quantization. Yi also offers extensions like a 200k context model, a \n",
            "vision-language model (Yi-VL), and a depth-upscaled 9B model.\n",
            "• Grok-138: Developed by xAI, Grok-1 is a 314B parameter Mixture-of-Experts model with \n",
            "25% of the weights active on a given token. It is the raw base model checkpoint from the \n",
            "pre-training phase and is not fine-tuned for specific tasks like dialogue. Grok-1 operates \n",
            "with a context length of 8k tokens.\n",
            "The pace of innovation with LLMs has been rapid and shows no signs of slowing down. There \n",
            "have been many contributions to the field in both the academic and commercial settings. \n",
            "With over 20,000 papers published about LLMs in arxiv.org it is impossible to name all \n",
            "of the models and teams that have contributed to the development of LLMs. However, an \n",
            "abbreviated list of open models of interest could include EleutherAI’s GPT-NeoX and GPT-J, \n",
            "Stanford’s Alpaca, Vicuna from LMSYS, Grok from xAI, Falcon from TII, PHI from Microsoft, \n",
            "NVLM from Nvidia, DBRX from Databricks, Qwen from Alibaba, Yi from 01.ai , Llama from \n",
            "Meta mentioned above and many others. Some of notable companies developing commercial \n",
            "foundation LLM models include Anthropic, Cohere, Character.ai, Reka, AI21, Perplexity, xAI \n",
            "and many others in addition to Google and OpenAI mentioned in previous sections. It is \n",
            "important when using a model to confirm that the license is appropriate for your use case as \n",
            "many models are provided with very specific terms of use.\n",
            "Comparison\n",
            "In this section, we observed how transformer-based language models have evolved. They \n",
            "started as encoder-decoder architectures with hundreds of millions of parameters trained \n",
            "on hundreds of millions of tokens, and have grown to be massive decoder-only architectures \n",
            "with billions of parameters and trained on trillions of tokens. Table 1 shows how the \n",
            "important hyperparameters for all the models discussed in this whitepaper have evolved \n",
            "Foundational Large Language Models & Text Generation35\n",
            "September 2024over time. The scaling of data and parameters has not only improved the performance of \n",
            "LLMs on downstream tasks, but has also resulted in emergent behaviors and zero- or few-\n",
            "shot generalizations to new tasks. However, even the best of these LLMs still have many \n",
            "limitations. For example, they are not good at engaging in human-like conversations, their \n",
            "math skills are limited, and they might not be aligned with human ethics (e.g., they might be \n",
            "biased or generate toxic responses). In the next section, we learn how a lot of these issues \n",
            "are being addressed.\n",
            "Foundational Large Language Models & Text Generation36\n",
            "September 2024Model\n",
            "Attention\n",
            "(2017)GPT  \n",
            "(2018)GPT-2\n",
            "(2019)GPT-3\n",
            "(2020)LaMDA  \n",
            "(2021)Gopher\n",
            "(2021)Chinchilla\n",
            "(2022)\n",
            "Optimizer ADAM ADAM ADAM ADAM ADAM ADAM ADAM-W\n",
            "# Parameters 213M 117M 1.5B 175B 137B 280B 70B\n",
            "Vocab size ~37K ~40K ~50K ~50K ~32K ~32K ~32K\n",
            "Embedding \n",
            "dimension 1024 768 1600 12288 8192 16384 8192\n",
            "Key dimension 64 64 64 128 128 128 128\n",
            "# heads (H) 16 12 25 96 128 128 64\n",
            "# encoder \n",
            "layers 6 N/A N/A N/A N/A N/A N/A\n",
            "# decoder \n",
            "layers 6 12 48 96 64 80 80\n",
            "Feed forward \n",
            "dimension 4 * 1024 4 * 768 4 * 1600 4 * 12288 8 * 8192 4 * 16384 4 * 8192\n",
            "Context Token \n",
            "SizeN/A 512 1024 2048 N/A 2048 2048\n",
            "Pre-Training \n",
            "tokens ~160MA ~1.25BA ~10B ~300B ~168B ~300B ~1.4T\n",
            "Table 1. Important hyperparameters for transformers-based large language models\n",
            "A. This number is an estimate based on the reported size of the dataset.\n",
            "Foundational Large Language Models & Text Generation37\n",
            "September 2024Fine-tuning large language models\n",
            "Large language models typically undergo multiple training stages. The first stage, often \n",
            "referred to as pre-training, is the foundational stage where an LLM is trained on large, \n",
            "diverse, and unlabelled text datasets where it’s tasked to predict the next token given the \n",
            "previous context. The goal of this stage is to leverage a large, general distribution of data \n",
            "and to create a model that is good at sampling from this general distribution. After language \n",
            "model pretraining, the resulting LLM usually demonstrates a reasonable level of language \n",
            "understanding and language generation skills across a variety of different tasks which \n",
            "are typically tested through zero-shot or few-shot prompting (augmenting the instruction \n",
            "with a few examples / demonstrations). Pretraining is the most expensive in terms of time \n",
            "(from weeks to months depending on the size of the model) and the amount of required \n",
            "computational resources, (GPU/TPU hours).\n",
            "After training, the model can be further specialized via fine-tuning, typically called \n",
            "instruction-tuning or simply supervised fine-tuning (SFT). SFT involves training an LLM on a \n",
            "set of task-specific demonstration datasets where its performance is also measured across \n",
            "a set of domain-specific tasks. The following are some examples of behaviors that can be \n",
            "improved using fine-tuning:\n",
            "• Instruction-tuning/instruction following: The LLM is provided as input an instruction to \n",
            "follow which might include summarizing a piece of text, writing a piece of code, or writing \n",
            "a poem in a certain style.17\n",
            "• Dialogue-tuning: This is a special case of instruction tuning where the LLM is fine-tuned \n",
            "on conversational data in the form of questions and responses. This is often called \n",
            "multi-turn dialogue.39\n",
            "Foundational Large Language Models & Text Generation38\n",
            "September 2024• Safety tuning: This is crucial for mitigating risks associated with bias, discrimination, and \n",
            "toxic outputs. It involves a multi-pronged approach encompassing careful data selection, \n",
            "human-in-the-loop validation, and incorporating safety guardrails. Techniques like \n",
            "reinforcement learning with human feedback (RLHF)40 enable the LLM to prioritize safe \n",
            "and ethical responses.\n",
            "Fine-tuning is considerably less costly and more data efficient compared to pre-training. \n",
            "Numerous techniques exist to optimize the costs further which are discussed later in \n",
            "this whitepaper.\n",
            "Supervised fine-tuning \n",
            "As mentioned in the previous section, SFT is the process of improving an LLM’s performance \n",
            "on a specific task or set of tasks by further training it on domain-specific, labeled data. The \n",
            "dataset is typically significantly smaller than the pre-training datasets, and is usually human-\n",
            "curated and of high quality. \n",
            "In this setting, each data point consists of an input (prompt) and a demonstration (target \n",
            "response). For example, questions (prompt) and answers (target response), translations from \n",
            "one language (prompt) to another language (target response), a document to summarize \n",
            "(prompt), and the corresponding summary (target response). \n",
            "It’s important to note that, while fine-tuning can be used to improve the performance on \n",
            "particular tasks as mentioned above, it can also serve the purpose of helping the LLM \n",
            "improve its behavior to be safer, less toxic, more conversational, and better at following \n",
            "instructions. \n",
            "Foundational Large Language Models & Text Generation39\n",
            "September 2024Reinforcement learning from human feedback\n",
            "Typically, after performing SFT, a second stage of fine-tuning occurs which is called \n",
            "reinforcement learning from human feedback  (RLHF). This is a very powerful fine-tuning \n",
            "technique that enables an LLM to better align with human-preferred responses (i.e. making \n",
            "its responses more helpful, truthful, safer, etc.). \n",
            "Figure 7. An example RLHF procedure \n",
            "In contrast to SFT, where an LLM is only exposed to positive examples (e.g. high-quality \n",
            "demonstration data), RLHF makes it possible to also leverage negative outputs thus \n",
            "penalizing an LLM when it generates responses that exhibit undesired properties. Penalizing \n",
            "negative output makes it less likely to generate unhelpful or unsafe responses. \n",
            "To leverage RLHF, a reward model  (RM) typically needs to be trained with a procedure similar \n",
            "to that in Figure 7. An RM is usually initialized with a pretrained transformer model, often also \n",
            "one that is SFT. Then it is tuned on human preference data which is either single sided (with a \n",
            "prompt, response and a score) or composed of a prompt and a pair of responses along with \n",
            "Foundational Large Language Models & Text Generation40\n",
            "September 2024a preference label indicating which of the two responses was preferred. For example, given \n",
            "two summaries, A and B, of the same article, a human rater selects a preferred summary \n",
            "(relying on the detailed guidance). We refer to the provided preference labels as human \n",
            "feedback. Preferences can be in the binary form (e.g. ‘good’ or ‘bad’), on the Likert scale42, \n",
            "rank order when more than 2 candidates are evaluated, or a more detailed assessment of the \n",
            "summary quality. The preference signal can also incorporate many dimensions that capture \n",
            "various aspects that define a high quality response, e.g., as safety, helpfulness, fairness, and \n",
            "truthfulness. \n",
            "Figure 7 shows a typical RLHF pipeline where a Reward model is initialized and finetuned on \n",
            "preference pairs. Once an RM has been trained, it’s then used by a Reinforcement Learning \n",
            "(RL)43 policy gradient algorithm, which further finetunes a previously instruction-tuned LLM to \n",
            "generate responses that are better aligned with human preferences. \n",
            "To better scale RLHF, RL from AI Feedback (RLAIF)44 leverages AI feedback instead of human \n",
            "feedback to generate preference labels. It’s also possible to remove the need for training \n",
            "RLHF by leveraging approaches such as direct preference optimization  (DPO).45 Both RLHF \n",
            "and RLAIF can be used on Google Cloud.\n",
            "Foundational Large Language Models & Text Generation41\n",
            "September 2024Parameter Efficient Fine-Tuning\n",
            "Both SFT and RLHF are still very costly in terms of compute time and accelerators required, \n",
            "especially when full-fine tuning entire LLMs on the orders of billions of parameters. Luckily, \n",
            "there are some really useful and effective techniques that can make fine-tuning significantly \n",
            "cheaper and faster compared to pre-training and full fine-tuning. One such family of \n",
            "methods is parameter efficient fine-tuning  (PEFT) techniques. \n",
            "At a high-level, PEFT approaches append a significantly smaller set of weights (e.g., on the \n",
            "order of thousands of parameters) that are used to ‘perturb’ the pre-trained LLM weights. \n",
            "The perturbation has the effect of fine-tuning the LLM to perform a new task or set of tasks. \n",
            "This has the benefit of training a significantly smaller set of weights, compared to traditional \n",
            "fine-tuning of the entire model. \n",
            "Some common PEFT techniques include the adapter, low-rank adaptation, and \n",
            "soft prompting:\n",
            "• Adapter-based fine-tuning46 employs small modules, called adapters, to the pre-\n",
            "trained model. Only the adapter parameters are trained, resulting in significantly fewer \n",
            "parameters than traditional SFT. \n",
            "• Low-Rank Adaptation  (LoRA)47 tackles efficiency differently. It uses two smaller matrices \n",
            "to approximate the original weight matrix update instead of fine-tuning the whole LLM. \n",
            "This technique freezes the original weights and trains these update matrices, significantly \n",
            "reducing resource requirements with minimum additional inference latency. Additionally, \n",
            "LoRA has improved variants such as QLoRA,48 which uses quantized weights for even \n",
            "greater efficiency. A nice advantage of LoRA modules is that they can be plug-and-play, \n",
            "meaning you can train a LoRA module that specializes in one task and easily replace it with \n",
            "another LoRA module trained on a different task. It also makes it easier to transfer the \n",
            "model since assuming the receiver has the original matrix, only the update matrices need \n",
            "to be provided.\n",
            "Foundational Large Language Models & Text Generation42\n",
            "September 2024• Soft prompting49 is a technique for conditioning frozen large language models with \n",
            "learnable vectors instead of hand-crafted text prompts. These vectors, called soft \n",
            "prompts, are optimized on the training data and can be as few as five tokens, making them \n",
            "parameter-efficient and enabling mixed-task inference. \n",
            "For most tasks, full fine-tuning is still the most performant, followed by LoRA and Soft \n",
            "prompting, but the order is reversed when it comes to cost. All three approaches are more \n",
            "memory efficient than traditional fine-tuning and achieve comparable performance.\n",
            "Foundational Large Language Models & Text Generation43\n",
            "September 2024Python\n",
            "# Before you start run this command:\n",
            "# pip install --upgrade --user --quiet google-cloud-aiplatform\n",
            "# after running pip install make sure you restart your kernel\n",
            "import vertexai\n",
            "from vertexai.generative_models import GenerativeModel\n",
            "from vertexai.preview.tuning import sft\n",
            "# TODO : Set values as per your requirements\n",
            "# Project and Storage Constants\n",
            "PROJECT_ID = ‘<project_id>’\n",
            "REGION = ‘<region>’\n",
            "vertexai.init(project=PROJECT_ID, location=REGION)\n",
            "# define training & eval dataset.\n",
            "TRAINING_DATASET = ‘gs://cloud-samples-data/vertex-ai/model-evaluation/\n",
            "peft_train_sample.jsonl’\n",
            "# set base model and specify a name for the tuned model\n",
            "BASE_MODEL = ‘gemini-1.5-pro-002’\n",
            "TUNED_MODEL_DISPLAY_NAME = ‘gemini-fine-tuning-v1’\n",
            "# start the fine-tuning job\n",
            "sft_tuning_job = sft.train(\n",
            "   source_model=BASE_MODEL,\n",
            "   train_dataset=TRAINING_DATASET,\n",
            "   # # Optional:\n",
            "   tuned_model_display_name=TUNED_MODEL_DISPLAY_NAME,\n",
            ")\n",
            "# Get the tuning job info.\n",
            "sft_tuning_job.to_dict()\n",
            "# tuned model endpoint name\n",
            "tuned_model_endpoint_name = sft_tuning_job.tuned_model_endpoint_name\n",
            "# use the tuned model\n",
            "tuned_genai_model = GenerativeModel(tuned_model_endpoint_name)\n",
            "print(tuned_genai_model.generate_content(contents= ’What is a LLM?’ ))\n",
            "Snippet 1. SFT fine tuning on Google cloud\n",
            "Foundational Large Language Models & Text Generation44\n",
            "September 2024Using large language models\n",
            "Prompt engineering and sampling techniques have a strong influence on the performance of \n",
            "LLMs. Prompt engineering is the process of designing and refining the text inputs (prompts) \n",
            "that you feed into an LLM to achieve desired and relevant outputs. Sampling techniques \n",
            "determine the way in which output tokens are chosen and influence the correctness, \n",
            "creativity and diversity of the resulting output. We next discuss different variants of prompt \n",
            "engineering and sampling techniques as well as touch on some important parameters that \n",
            "can have a significant impact on LLM performance.\n",
            "Prompt engineering \n",
            "LLMs are very powerful, but they still need guidance to unleash their full potential. Prompt \n",
            "engineering is a critical component in guiding an LLM to yield desired outputs. This might \n",
            "include grounding the model to yield factual responses or unleashing the creativity of the \n",
            "model to tell a story or write a song. Examples of prompt engineering include providing \n",
            "clear instructions to the LLM, giving examples, using keywords, and formatting to emphasize \n",
            "important information, providing additional background details etc. \n",
            "You will often hear the terms zero-shot, few-shot, and chain-of-thought prompting in the \n",
            "context of prompt engineering. We define these terms below: \n",
            "• Few-shot prompting: This is when you provide the LLM with a task description, as well \n",
            "as a few (e.g. three to five) carefully chosen examples, that will help guide the LLM’s \n",
            "response. For example, you might provide the model with the name of a few countries \n",
            "and their capital cities, then ask it to generate the capital for a new country that isn’t in \n",
            "the examples.\n",
            "Foundational Large Language Models & Text Generation45\n",
            "September 2024• Zero-shot prompting: This is when you provide the LLM directly with a prompt with \n",
            "instructions. You usually give the LLM a task description and the LLM relies heavily on its \n",
            "existing knowledge to output the correct response. This requires no additional data or \n",
            "examples, hence the name ‘Zero-shot’ but can be less reliable than few-shot prompting.\n",
            "• Chain-of-thought prompting: This technique aims to improve performance on complex \n",
            "reasoning tasks. Rather than simply asking the LLM a question, you provide a prompt \n",
            "that demonstrates how to solve similar problems using step-by-step reasoning. The \n",
            "LLM then generates its own chain of thought for the new problem, breaking it down into \n",
            "smaller steps and explaining its reasoning. Finally, it provides an answer based on its \n",
            "reasoning process.\n",
            "Prompt engineering is an active area of research.\n",
            "Sampling Techniques and Parameters\n",
            "A variety of sampling techniques can be employed to determine how the model chooses \n",
            "the next token in a sequence. They are essential for controlling the quality, creativity, and \n",
            "diversity of the LLM’s output. The following is a breakdown of different sampling techniques \n",
            "and their important parameters:\n",
            "• Greedy search50: Selects the token with the highest probability at each step. This is the \n",
            "simplest option but it can lead to repetitive and predictable outputs.\n",
            "• Random sampling :50 Selects the next token according to the probability distribution, where \n",
            "each token is sampled proportionally to its predicted probability. This can produce more \n",
            "surprising and creative text, but also a higher chance of nonsensical output.\n",
            "• Temperature sampling :50 Adjusts the probability distribution by a temperature parameter. \n",
            "Higher temperatures promote diversity, lower temperatures favor high-probability tokens.\n",
            "Foundational Large Language Models & Text Generation46\n",
            "September 2024• Top-K sampling: Randomly samples from the top K most probable tokens. The value of K \n",
            "controls the degree of randomness.\n",
            "• Top-P sampling (nucleus sampling):51 Samples from a dynamic subset of tokens whose \n",
            "cumulative probability adds up to P. This allows the model to adapt the number of potential \n",
            "candidates depending on its confidence, favoring more diversity when uncertain and \n",
            "focusing on a smaller set of highly probable words when confident.\n",
            "• Best-of-N sampling: Generates N separate responses and selects the one deemed best \n",
            "according to a predetermined metric (e.g., a reward model or a logical consistency check). \n",
            "This is particularly useful for short snippets or situations where logic and reasoning \n",
            "are key.\n",
            "By combining prompt engineering with sampling techniques and correctly calibrated \n",
            "hyperparameters, you can greatly influence the LLM’s response, making it more relevant, \n",
            "creative, and consistent for your specific needs.\n",
            "Until now, we have seen the various types of LLM architectures, their underlying technology, \n",
            "as well as the approaches used to train, tune, and adapt these models for various tasks. Let’s \n",
            "now look at some key research about how the decoding process in LLMs can be sped up \n",
            "considerably to generate faster responses. \n",
            "Accelerating inference\n",
            "The scaling laws for LLMs which were initially explored by the Kaplan et al.24 study continue \n",
            "to hold today. Language models have been consistently increasing in size and this has been \n",
            "a direct contributor to the vast improvement in these models’ quality and accuracy over the \n",
            "last few years. As increasing the number of parameters has improved the quality of LLMs it \n",
            "Foundational Large Language Models & Text Generation47\n",
            "September 2024has also increased the computational resources needed to run them. Numerous approaches \n",
            "have been used to try and improve the efficiency of LLMs for different tasks as developers \n",
            "are incentivized to reduce cost and latency for model users. Balancing the expense of \n",
            "serving a model in terms of time, money, energy is known as the cost-performance tradeoff \n",
            "and often needs adjusting for particular use cases.\n",
            "Two of the main resources used by LLMs are memory and computation. Techniques for \n",
            "improving the efficiency or speed of inference focus primarily on these resources. The \n",
            "speed of the connection between memory and compute is also critical, but usually hardware \n",
            "constrained.  As LLMs have grown in size 1000x from millions to billions of parameters. \n",
            "Additional parameters increase both the size of memory required to hold the model and \n",
            "computations needed to produce the model results.\n",
            "With LLMs being increasingly adopted for large-scale and low-latency use cases, finding \n",
            "ways to optimize their inference performance has become a priority and an active research \n",
            "topic with significant advancements. We will explore a number of methods and a few \n",
            "tradeoffs for accelerating inference.\n",
            "Trade offs\n",
            "Many of the high yielding inference optimisation methods mandate trading off a number of \n",
            "factors, this can be tweaked on a case-by-case basis allowing for tailored approaches to \n",
            "different inference use cases and requirements. A number of the optimization methods we \n",
            "will discuss later fall somewhere on the spectrum of these tradeoffs. \n",
            "Foundational Large Language Models & Text Generation48\n",
            "September 2024Trading off one factor against the other (e.g. latency vs quality or cost) doesn’t mean that \n",
            "we’re completely sacrificing that factor, it just means that we’re accepting what might be \n",
            "a marginal degradation in quality, latency or cost for the benefit of substantially improving \n",
            "another factor.\n",
            "The Quality vs Latency/Cost Tradeoff\n",
            "It is possible to improve the speed and cost of inference significantly through accepting \n",
            "what might be marginal to negligible drops in the model’s accuracy. One  example of this \n",
            "is using a smaller model to perform the task. Another example is quantisation where we \n",
            "decrease the precision of the model’s parameters thereby leading to faster and less memory \n",
            "intensive calculations.\n",
            "One important distinction when approaching this trade-off is between the theoretical \n",
            "possibility of a quality loss versus the practical capability of the model to perform the desired \n",
            "task. This is use case specific and exploring it will often lead to significant speedups without \n",
            "sacrificing quality in a meaningful or noticeable way. For example, if the task we want the \n",
            "model to perform is simple, then a smaller model or a quantised one will likely be able to \n",
            "perform this task well. Reduction in parametric capacity or precision does not automatically \n",
            "mean that the model is less capable at that specific task.\n",
            "The Latency vs Cost Tradeoff\n",
            "Another name for this tradeoff is the latency vs throughput tradeoff. Where throughput refers \n",
            "to the system’s ability at handling multiple requests efficiently. Better throughput on the same \n",
            "hardware means that our LLM inference cost is reduced, and vice versa.\n",
            "Foundational Large Language Models & Text Generation49\n",
            "September 2024Much like traditional software systems, there are often multiple opportunities to tradeoff \n",
            "latency against the cost of LLM inference. This is an important tradeoff since LLM inference \n",
            "tends to be the slowest and most expensive component in the entire stack; balancing latency \n",
            "and cost intentionally is key to making sure we tailor LLM performance to the product or use \n",
            "case it’s being used in. An example would be bulk inference use cases (e.g. offline labeling) \n",
            "where cost can be a more important factor than the latency of any particular request. On the \n",
            "other hand, an LLM chatbot product will place much higher importance on request latency.\n",
            "Now that we’ve covered some of the important tradeoffs to consider when optimizing \n",
            "inference, let’s examine some of the most effective inference acceleration techniques. As \n",
            "discussed in the tradeoffs section, some optimization techniques can have an impact on the \n",
            "model’s output. Therefore we will split the methods into two types: output-approximating \n",
            "and output-preserving.\n",
            "Output-approximating methods\n",
            "Quantization\n",
            "LLMs are fundamentally composed of multiple numerical matrices (a.k.a the model weights). \n",
            "During inference, matrix operations are then applied to these model weights to produce \n",
            "numerical outputs (a.k.a activations). Quantization is the process of decreasing the numerical \n",
            "precision in which weights and activations are stored, transferred and operated upon. The \n",
            "default representation of weights and activations is usually 32 bits floating numbers, with \n",
            "quantization we can drop the precision to 8 or even 4 bit integers. \n",
            "Foundational Large Language Models & Text Generation50\n",
            "September 2024Quantization has multiple performance benefits, it reduces the memory footprint of \n",
            "the model, allowing to fit larger models on the same hardware, it also reduces the \n",
            "communication overhead of weights and activations within one chip and across chips in \n",
            "a distributed inference setup- therefore speeding up inference as communication is a \n",
            "major contributor to latency. In addition, decreasing the precision of weights/activations \n",
            "can enable faster arithmetic operations on these models as some accelerator hardware \n",
            "(e.g. TPUs/GPUs) natively supports faster matrix multiplication operations for some lower \n",
            "precision representations.\n",
            "Quantization’s impact on quality can be very mild to non-existent depending on the use \n",
            "case and model.  Further, in cases where quantisation might introduce a quality regression, \n",
            "that regression can be small compared to the performance gain, therefore allowing for an \n",
            "effective Quality vs Latency/Cost Tradeoff. For example, Benoit Jacob et al.55 reported a 2X \n",
            "speed-up for a 2% drop in accuracy for the FaceDetection task on MobileNet SSD.\n",
            "Quantization can be either applied as an inference-only operation, or it can be incorporated \n",
            "into the training (referred to as Quantisation Aware Training QAT). QAT is generally \n",
            "considered to be a more resilient approach as the model is able to recover some of the \n",
            "quantisation-related quality losses during training. To make sure we get the best cost/quality \n",
            "tradeoff, we tweak the quantization strategy (e.g. select different precisions for weights \n",
            "vs activations) and the granularity in which we apply quantisation to Tensors (e.g. channel \n",
            "or group-wise58).\n",
            "Distillation\n",
            "Using a smaller model to perform a task is one of the most efficient inference optimization \n",
            "techniques, however, smaller models can demonstrate significant regressions on quality \n",
            "compared to their larger counterparts.\n",
            "Foundational Large Language Models & Text Generation51\n",
            "September 2024Distillation is a set of training techniques that targets improving the quality of a smaller model \n",
            "(the student) using a larger model (the teacher). This method can be effective because larger \n",
            "models outperform smaller ones even if both are trained on the same data, mainly due to \n",
            "parametric capacity and training dynamics. The gap in performance continues as the training \n",
            "dataset grows as illustrated by Figure 8.\n",
            "It is worth noticing that even at low volumes of training data, large models can already \n",
            "demonstrate better performance than the correspondingly trained smaller models, this fact \n",
            "leads us to the first variant of distillation which is referred to as data distillation or model \n",
            "compression.56 We use a large model which was trained on the data we have to generate \n",
            "more synthetic data to train the smaller student model, the increase in data volume will help \n",
            "move the the student further along the quality line compared to only training on the original \n",
            "data. Synthetic data needs to be approached carefully as it needs to be of high quality and \n",
            "can lead to negative effects otherwise.\n",
            "Figure 8. An illustration of the performance of models of various sizes as a function of the training \n",
            "dataset’s size\n",
            "Foundational Large Language Models & Text Generation52\n",
            "September 2024Other distillation techniques attempt to bring the student model closer to the teacher \n",
            "on a more granular level than just synthetic data generation. One prominent technique is \n",
            "knowledge distillation57, in this approach we attempt to align the output token distribution \n",
            "of the student model to that of the teacher’s, this can be much more sample efficient than \n",
            "data distillation. On-policy distillation59 is another technique that leverages feedback from \n",
            "the teacher model on each sequence generated by the student in a reinforcement learning \n",
            "setup. \n",
            "Output-preserving methods\n",
            "These methods are guaranteed to be quality neutral, they cause no changes to the model \n",
            "output which often makes them obvious first steps to optimize inference before facing the \n",
            "more nuanced tradeoffs of the approximating methods\n",
            "Flash Attention\n",
            "Scaled Dot-product Attention, which is the predominant attention mechanism in the \n",
            "transformer architecture, is a quadratic operation on the input length. Optimizing the self-\n",
            "attention calculation can bring significant latency and cost wins.\n",
            "Flash Attention, introduced in by Tri Dao et al.62, optimizes the attention calculation by making \n",
            "the attention algorithm IO Aware, particularly trying to minimize the amount of data we move \n",
            "between the slow HBM (high bandwidth memory) to the faster memory tier (SRAM/VMEM) in \n",
            "TPUs and GPUs. When calculating attention, the order of operations is changed and multiple \n",
            "layers are fused so we can utilize the faster memory tiers as efficiently as possible.\n",
            "Foundational Large Language Models & Text Generation53\n",
            "September 2024Flash Attention is an exact algorithm, it maintains the numerical output of the attention \n",
            "computation and can yield significant latency benefits due to reducing the IO overhead, Tri \n",
            "Dao et al.62 showed 2-4X latency improvements in the attention computation.\n",
            "Prefix Caching\n",
            "One of the most compute intensive, and thus slowest, operations in LLM inference is \n",
            "calculating the attention key and value scores (a.k.a KV) for the input we’re passing to the \n",
            "LLM, this operation is often referred to as prefill. The final output of prefill is what is termed \n",
            "KV Cache which includes the attention key and value scores for each layer of the transformer \n",
            "for the entire input. This cache is vital during the decoding phase which produces the output \n",
            "tokens, the KV cache allows us to avoid recalculating attention scores for the input on each \n",
            "autoregressive decode step.\n",
            "Prefix Caching refers to the process of caching the KV Cache itself between subsequent \n",
            "inference requests in order to reduce the latency and cost of the prefill operation. The way \n",
            "the self-attention mechanism works makes reusing KV caches possible because tokens will \n",
            "only pay attention to tokens that came before them in the sequence. If there’s new input \n",
            "being appended to input that the model has seen before, then we can potentially avoid \n",
            "recalculating the prefill for the older input.\n",
            "Foundational Large Language Models & Text Generation54\n",
            "September 2024Figure 9. An illustration of Prefix Caching in a chat scenario\n",
            "Figure 9 illustrates how prefix caching works in a multi-turn scenario with a document upload. \n",
            "On the first user turn, the prefill operation has to process the entire document therefor taking \n",
            "500ms, the resulting KV cache is then stored so that on the second user turn, we can retrieve \n",
            "the cache directly from storage and avoid recomputing it for the long doc, therefore saving \n",
            "substantial amounts of compute and latency.\n",
            "\n",
            "Foundational Large Language Models & Text Generation55\n",
            "September 2024Prefix caches can be stored either in memory or on disk and fetched on-demand. One \n",
            "important consideration is making sure that the input structure/schema remains prefix-\n",
            "caching friendly, we should avoid changing the prefix in subsequent requests as that will \n",
            "invalidate the cache for all the tokens that follow For example, putting a fresh timestamp at \n",
            "the very beginning of each request will invalidate the cache completely as every subsequent \n",
            "request will have a new prefix.\n",
            "Many LLM use cases lend themselves naturally to prefix caching. For example, LLM Chatbots \n",
            "where users will have a multi-turn conversation that can span 10s of 1000s of tokens and \n",
            "we can avoid recalculating the KV cache for the previous parts of the conversation. Large \n",
            "document/code uploads is another use case where the artifact the user uploads will remain \n",
            "unchanged from one request to the next. All that’s changing are the questions the user is \n",
            "asking, so caching the KV cache for the document (especially for larger artifacts) can result \n",
            "in significant latency and cost savings.\n",
            "Prefix caching is available as a service called Context Caching on Google AI studio52 and  \n",
            "Vertex AI on Google Cloud53.\n",
            "Speculative Decoding\n",
            "The first phase of LLM inference, known as prefill, is compute bound due large matrix \n",
            "operations on many tokens occurring in parallel. The second phase, known as decode, is \n",
            "generally memory bound as tokens are auto-regressively decoded one at a time. \n",
            "Foundational Large Language Models & Text Generation56\n",
            "September 2024It is not easy to naively use additional parallel compute capacity to speed up decode \n",
            "given the  need to wait for the current token to be produced before we can calculate what \n",
            "the next token should be (as per the self-attention mechanism), the decode process is \n",
            "inherently serial.\n",
            "Speculative decoding (Leviathan at al.63) aims to overcome this limitation in decode by finding \n",
            "a way to utilize the spare compute capacity to make each decode step faster. The main idea \n",
            "is to use a much smaller secondary model (often referred to as the drafter) to run ahead of \n",
            "the main model and predict more tokens. (e.g. 4 tokens ahead). This will happen very quickly \n",
            "as the drafter is much faster and smaller than the main model. We then use the main model to \n",
            "verify the hypotheses of the drafter in parallel for each of the 4 steps (i.e. the first token, the \n",
            "first two tokens, the first 3 tokens and finally all 4 tokens), and we then select the accepted \n",
            "hypothesis with the maximum number of tokens. For example:\n",
            "Figure 10. An illustration of speculative decoding over 3 tokens\n",
            "Note that the 3 main model steps run in parallel. And because we are not compute bound in \n",
            "decode, we can use the spare capacity to get much better decode latencies. In the example \n",
            "above, let’s say a single main model step needs 10ms, while the drafter needs 1ms. Without \n",
            "speculative decoding, we need 3 * 10ms = 30ms to produce the response, with speculative \n",
            "\n",
            "Foundational Large Language Models & Text Generation57\n",
            "September 2024decoding, there’s only one main model step on the critical path due to parallelization, so we \n",
            "need 3 * 1ms + 10ms = 13ms. A significant latency improvement. This technique is completely \n",
            "quality neutral, the main model will reject any tokens that it wouldn’t have predicted itself \n",
            "in the first place, so the only thing speculative decoding does is run ahead and present \n",
            "hypotheses that the main model can accept or reject in parallel.\n",
            "One important condition for speculative decoding to work effectively is that the drafter model \n",
            "has good levels of alignment with the main model, otherwise we won’t be able to accept any \n",
            "of the tokens. So investing in the training quality of the drafter model is worthwhile to get \n",
            "better latencies.\n",
            "Now that we have seen some methods to make LLM generate their responses faster, let’s \n",
            "look at some examples of how these models can be applied to various tasks to get an idea \n",
            "how to use them.\n",
            "Batching and Parallelization\n",
            "Most of the optimization techniques we’ve discussed so far are specific to Machine Learning \n",
            "and Transformer architecture in particular. However, much like any software system, there \n",
            "are opportunities to improve throughput and latency through a combination of 1) batching \n",
            "less compute-intensive operations (i.e. we can run multiple requests on the same hardware \n",
            "simultaneously to better utilize the spare compute) and 2) parallelizing the more compute-\n",
            "intensive parts of the computations (i.e. we can divide the computation and split it amongst \n",
            "more hardware instances to get more compute capacity and therefore better latencies\n",
            "Batching in LLMs is most useful on the decode side - as we explained in the Speculative \n",
            "Decoding section, decode is not compute-bound and therefore there’s an opportunity \n",
            "to batch more requests. We need to be careful that we batch computations in a way that \n",
            "Foundational Large Language Models & Text Generation58\n",
            "September 2024enables utilization of the spare capacity which is possible to do on accelerators (e.g. TPUs \n",
            "and GPUs). We also need to make sure we remain within the memory limits, as decode is a \n",
            "memory intensive operations, batching more requests will put more pressure on the free \n",
            "memory available. Batching has become an important component in most high-throughput \n",
            "LLM inference setups.\n",
            "Parallelization is also a widely used technique given the variety of opportunities in \n",
            "transformers for horizontal scaling across more hardware instances. There are multiple \n",
            "parallelism techniques across the model input (Sequence parallelism) the model layers \n",
            "(Pipeline parallelism), and within a single layer (Tensor parallelism). One of the most important \n",
            "considerations for parallelism is the cost of communication and synchronization between \n",
            "the different shards that we distribute to other machines. Communication is a significant \n",
            "overhead and can erode the benefits of adding more computational capacity if we’re not \n",
            "careful about which parallelization strategy to use. On the other hand, selecting the right \n",
            "strategy to balance the need for additional compute and the communication cost can yield \n",
            "significant latency wins.\n",
            "Now that we have seen some methods to make LLM generate their responses faster, let’s \n",
            "look at some examples of how these models can be applied to various tasks to get an idea \n",
            "how to use them.\n",
            "Applications\n",
            "Large language models are revolutionizing the way we interact with and process information. \n",
            "With their unprecedented ability to understand context and generate content, they’re \n",
            "transforming numerous applications in the worlds of text, code, images, audio and video. \n",
            "Here we collected a few examples of application areas, but the reader should keep in mind \n",
            "that this is not a comprehensive list and that many new ideas are emerging continuously \n",
            "Foundational Large Language Models & Text Generation59\n",
            "September 2024about how to best utilize the capabilities of these new tools. For more information about \n",
            "optimally building and deploying functioning applications based on the following mentioned \n",
            "use cases, refer to the subsequent whitepapers. \n",
            "It is also very simple to generate text-based responses for your use case using either \n",
            "the Google Cloud Vertex AI SDK or the Developer focused AI studio. Snippet 3 shows \n",
            "code examples from these SDKs to generate responses to text prompts using the Gemini \n",
            "model. Note that the multimodal aspects of Gemini are covered in their respective \n",
            "dedicated whitepapers.\n",
            "Foundational Large Language Models & Text Generation60\n",
            "September 2024Python\n",
            "# Before you start run this command:\n",
            "# pip install --upgrade --user --quiet google-cloud-aiplatform\n",
            "# after running pip install make sure you restart your kernel\n",
            "import vertexai\n",
            "from vertexai.language_models import TextGenerationModel\n",
            "from vertexai.preview.generative_models import GenerationConfig,GenerativeModel\n",
            "# Set values as per your requirements\n",
            "PROJECT_ID = ‘<project_id>’  # set to your project_id\n",
            "vertexai.init(project=PROJECT_ID, location= ’us-central1’ )\n",
            "PROMPT= ‘What is a LLM?’  # set your prompt here\n",
            "model = GenerativeModel( ‘gemini-1.5-pro-002’ )\n",
            "# call the Gemini API\n",
            "response = model.generate_content(\n",
            "   PROMPT)\n",
            "print(response.text)\n",
            "# google AI Studio SDK\n",
            "import google.generativeai as genai\n",
            "import os\n",
            "# update with your API key\n",
            "genai.configure(api_key=os.environ[ “GOOGLE_API_KEY” ])\n",
            "# choose the mode l\n",
            "model = genai.GenerativeModel( ‘gemini-pro’ )\n",
            "response = model.generate_content( ‘What is a LLM?’ ) # set your prompt here\n",
            "print(response.text)\n",
            "Snippet 3. Using Vertex AI and Google AI studio SDKs for unimodal text gene \n",
            "Foundational Large Language Models & Text Generation61\n",
            "September 2024Code and mathematics\n",
            "Generative models can comprehend and generate code and algorithms to supercharge \n",
            "developers by assisting them across many application areas. Some of the popular use cases \n",
            "for code include:\n",
            "• Code generation: LLMs can be prompted in natural language to generate code in a \n",
            "specific programming language to perform certain operations. The output can be used as \n",
            "a draft.\n",
            "• Code completion:  LLMS can proactively suggest useful code as the user types it. This \n",
            "can save developers time and improve code quality.\n",
            "• Code refactoring and debugging:  LLMs can help reduce technical debt by refactoring \n",
            "and debugging code to improve quality, efficiency and correctness.\n",
            "• Code translation:  LLMs can significantly help developer time and effort by helping to \n",
            "convert code from one programming language to another. For example, an LLM might \n",
            "convert Python code to Java.\n",
            "• Test case generation: LLMs can be prompted to generate unit tests for a provided \n",
            "codebase which saves considerable time and reduces errors.\n",
            "• Code documentation and understanding: LLMs can be used in a conversational manner \n",
            "to engage in a natural language chat to help you understand a codebase. They can also \n",
            "generate appropriate comments, understand copyright status, and create release notes.\n",
            "Recently, a number of exciting advancements have been made in the space of competitive \n",
            "coding and mathematics. AlphaCode 2 ,64 combines Gemini’s reasoning capabilities with \n",
            "search and the use of tools to solve competitive coding problems. It receives as input a \n",
            "description of a problem to solve, and outputs a code solution that solves the problem. It \n",
            "Foundational Large Language Models & Text Generation62\n",
            "September 2024now ranks among the top 15% competitive coders on the popular Codeforces competitive \n",
            "coding platform. FunSearch65 uses an evolutionary procedure which is based on pairing \n",
            "a pre-trained LLM with a systematic evaluator. It solved the cap set problem66, an open \n",
            "problem in mathematics, and also discovered more efficient bin-packing algorithms which \n",
            "are used in many applications such as making data centers more efficient. Another recent \n",
            "approach called AlphaGeometry  tackles the problem of finding proofs for complex geometric \n",
            "theorems. It comprises a neuro-symbolic system made up of a neural language model and \n",
            "a symbolic deduction engine. AlphaGeometry managed to solve 25 out of 30 Olympiad \n",
            "geometry problems, where the average human gold medalist scores on average 25.9. 67\n",
            "Machine translation\n",
            "LLMs are capable of generating fluid, high-quality and contextually accurate translations. \n",
            "This is possible due to the LLM’s deep understanding of linguistic nuances, idioms, and \n",
            "context. The following are some possible real world use cases:\n",
            "• Instant messaging apps: In messaging platforms, LLMs can provide on-the-fly \n",
            "translations that feel natural. Unlike previous algorithms that might translate word-\n",
            "for-word, LLMs understand slang, colloquialisms, and regional differences, enhancing \n",
            "cross-language communication.\n",
            "• E-commerce: On global platforms like AliExpress, product descriptions are automatically \n",
            "translated. LLMs help with ensuring cultural nuances and idiomatic expressions in product \n",
            "details are appropriately translated, leading to fewer misunderstandings.\n",
            "• Travel apps:  In apps like Google Translate, travelers get real-time spoken translations. \n",
            "With LLMs, the translated conversations are smoother, making interactions in foreign \n",
            "countries more effortless.\n",
            "Foundational Large Language Models & Text Generation63\n",
            "September 2024Text summarization\n",
            "Text summarization is a core capability of many of the LLMs mentioned in this whitepaper. \n",
            "There are a number of natural potential use cases which include:\n",
            "• News aggregators: LLMs could craft summaries that capture not only the main \n",
            "events but also the sentiment and tone of the article, providing readers with a more \n",
            "holistic understanding.\n",
            "• Research databases: LLMs could help researchers generate abstracts that encapsulate \n",
            "the core findings and implications of scientific papers.\n",
            "• Chat management:  In platforms like Google Chat, LLM-based systems could generate \n",
            "thread summaries that capture the urgency and tone, aiding users in prioritizing \n",
            "their responses.\n",
            "Question-answering\n",
            "The older generation of QA systems often worked by keyword matching, frequently missing \n",
            "out on the contextual depth of user queries. LLMs, however, dive deep into context. They can \n",
            "infer user intent, traverse vast information banks, and provide answers that are contextually \n",
            "rich and precise. Some of the examples where this could be used include:\n",
            "• Virtual assistants: LLMs can offer detailed explanations of a weather forecast \n",
            "considering the user’s location, time of year, and recent weather trends.\n",
            "• Customer support: In business platforms, LLM-based bots could provide answers that \n",
            "take into account the user’s purchase history, past queries, and potential issues, offering \n",
            "personalized assistance.\n",
            "Foundational Large Language Models & Text Generation64\n",
            "September 2024• Academic platforms: On academic platforms like Wolfram Alpha, LLMs could cater to \n",
            "user queries by understanding the depth and context of academic questions, offering \n",
            "answers that suit everyone from a high school student to a postgraduate researcher.\n",
            "The quality of the generated answers, as well as the corresponding citations and sources \n",
            "can be significantly improved by using advanced search systems (such as those based on \n",
            "Retrieval Augmented Generation (RAG) architectures) to expand the prompt with relevant \n",
            "information, as well as post-hoc grounding after the response has been generated. Clear \n",
            "instructions, roles of what should and should not be used to answer the question, and \n",
            "advanced prompt engineering approaches (such as chain of thought and search/RAG \n",
            "architectures), combined with a lower temperature value amongst other things can also \n",
            "help greatly.\n",
            "Chatbots\n",
            "Earlier chatbots followed scripted pathways, leading to ‘mechanical’ conversations. LLMs \n",
            "transform this space by offering dynamic, human-like interactions. They can analyze \n",
            "sentiment, context, and even humor, making digital conversations feel more authentic. Some \n",
            "examples of where this can be used include:\n",
            "• Customer service: A chatbot on retail platforms like Zara could not only answer product-\n",
            "related queries but also offer fashion advice based on current trends.\n",
            "• Entertainment: On Media LLM-driven chatbots could engage with users dynamically, \n",
            "reacting to live events in the stream and moderating chats with contextual understanding.\n",
            "Foundational Large Language Models & Text Generation65\n",
            "September 2024Content generation\n",
            "Text generation isn’t new, but what LLMs bring to the table is the unprecedented ability \n",
            "to generate human-like text that’s contextually relevant and rich in detail. Earlier models \n",
            "would often lose context or coherence over longer passages. LLMs, with their vast \n",
            "knowledge and nuanced understanding, can craft text spanning various styles, tones, and \n",
            "complexities, mixing factuality with creativity (depending on the context) effectively bridging \n",
            "the gap between machine-generated and human-written content. The following are some \n",
            "real-world examples:\n",
            "• Content creation: Platforms could utilize LLMs to help marketers develop advertisements. \n",
            "Instead of generic content, the LLMs could generate creative, targeted, and \n",
            "audience-specific messages.\n",
            "• Scriptwriting:  LLMs could potentially assist with producing scripts for movies or TV \n",
            "shows. Writers could input themes or plot points, and the model can suggest dialogues or \n",
            "scene descriptions, enhancing the creative process.\n",
            "Text generation is a wide task encompassing a variety of use cases that might range from \n",
            "those where correctness of the generated output is more or less important than its creativity/\n",
            "diversity of the language. The sampling methods and parameters like temperature should be \n",
            "tuned accordingly. For more information, see the prompt engineering and architecting for \n",
            "LLM applications whitepapers.\n",
            "Natural language inference\n",
            "Natural language inference  (NLI) is the task of determining whether a given textual \n",
            "hypothesis can be logically inferred from a textual premise.\n",
            "Foundational Large Language Models & Text Generation66\n",
            "September 2024Traditional models struggled with nuanced relationships or those that require a deeper \n",
            "understanding of context. LLMs, with their intricate grasp of semantics and context, excel \n",
            "at tasks like these, bringing accuracy levels close to human performance. The following are \n",
            "some real-world examples:\n",
            "• Sentiment analysis:  Businesses could utilize LLMs to infer customer sentiment from \n",
            "product reviews. Instead of just basic positive or negative tags, they could extract \n",
            "nuanced emotions like ‘satisfaction,’ ‘disappointment,’ or ‘elation’.\n",
            "• Legal document review: Law firms could employ LLMs to infer implications \n",
            "and intentions in contracts, ensuring there are no contradictions or potentially \n",
            "problematic clauses.\n",
            "• Medical diagnoses: By analyzing patient descriptions and histories, LLMs could assist \n",
            "doctors in inferring potential diagnoses or health risks, ensuring early intervention.\n",
            "The whitepapers on domain specific LLMs, prompt engineering, and architecting for LLM \n",
            "applications give further insight into these use cases.\n",
            "Text classification\n",
            "Text classification involves categorizing text into predefined groups. While traditional \n",
            "algorithms were efficient, they often struggled with ambiguous or overlapping categories. \n",
            "LLMs, given their deep understanding of context, can classify text with higher precision, even \n",
            "when faced with subtle distinctions. Some examples of this include:\n",
            "• Spam detection: Email services could utilize LLMs to classify emails as spam or \n",
            "legitimate. Instead of just keyword-based detection, the models understand the context \n",
            "and intent, potentially reducing false positives.\n",
            "Foundational Large Language Models & Text Generation67\n",
            "September 2024• News categorization: News platforms could employ LLMs to categorize articles into \n",
            "topics like ‘technology,’ ‘politics,’ or ‘sports,’ even when articles blur the boundaries \n",
            "between categories.\n",
            "• Customer feedback sorting: Businesses could analyze customer feedback through \n",
            "LLMs to categorize them into areas like ‘product design,’ ‘customer service,’ or ‘pricing,’ \n",
            "ensuring targeted responses.\n",
            "• Evaluating LLMs as autorater:  LLMs could be used to rate, compare and rank the \n",
            "generated outputs of other LLMs as well.\n",
            "Text analysis\n",
            "LLMs excel at deep text analysis – extracting patterns, understanding themes, and gleaning \n",
            "insights from vast textual datasets. Where traditional tools would scratch the surface, LLMs \n",
            "delve deep, offering rich and actionable insights. Some potential real-world examples are:\n",
            "• Market research: Companies could leverage LLMs to analyze consumer conversations on \n",
            "social media, extracting trends, preferences, and emerging needs.\n",
            "• Literary analysis: Academics could employ LLMs to understand themes, motifs, and \n",
            "character developments in literary works, offering fresh perspectives on classic and \n",
            "contemporary literature.\n",
            "Foundational Large Language Models & Text Generation68\n",
            "September 2024Multimodal applications\n",
            "Multimodal LLMs, capable of processing and generating text, images, audio, and video, have \n",
            "opened up a new frontier in AI, offering a range of exciting and innovative applications across \n",
            "various sectors. The following are some examples: \n",
            "Creative content generation:\n",
            "• Storytelling: An AI system could watch an image or video and spin a captivating narrative, \n",
            "integrating details from the visual with its knowledge base.\n",
            "• Advertising and marketing: Generating targeted and emotionally resonant advertisements \n",
            "based on product photos or videos.\n",
            "Education and accessibility:\n",
            "• Personalized learning: Tailoring educational materials to individual learning styles by \n",
            "combining text with interactive visual and audio elements.\n",
            "• Assistive technology: Multimodal LLMs could power tools that describe images, videos, \n",
            "and audio for visually or hearing-impaired individuals.\n",
            "Business and industry:\n",
            "• Document understanding and summarization: Automatically extracting key information \n",
            "from complex documents, combining text and visuals like invoices and contracts.\n",
            "• Customer service: Multimodal chatbots can understand and respond to customer queries \n",
            "combining text and images, offering a richer and more personalized experience. Science \n",
            "and research:\n",
            "Foundational Large Language Models & Text Generation69\n",
            "September 2024• Medical diagnosis: Analyzing medical scans and reports together, identifying potential \n",
            "issues and providing insights for doctors.\n",
            "• Bioinformatics and drug discovery: Integrating knowledge from diverse data sources like \n",
            "medical images, protein structures, and research papers to accelerate research.\n",
            "These examples are just the tip of the iceberg. As research progresses, the applications \n",
            "of multimodal LLMs are only expected to grow, transforming our daily lives in diverse and \n",
            "profound ways. Multimodal LLMs also benefit greatly from the existing methodologies of \n",
            "Unimodal LLMs ( i.e., text based LLMs).\n",
            "LLMs, thanks to their ability to understand and process language, are reshaping how we \n",
            "interact with, generate, and analyze text across diverse sectors. As they continue to evolve, \n",
            "their applications will only grow, boosting the ability for machines and humans to have rich \n",
            "natural language interactions.\n",
            "Summary\n",
            "In this whitepaper we have discussed the basics of transformers, upon which all modern-day \n",
            "LLMs are based. We detailed the evolution of the various LLM model architectures and their \n",
            "components. We’ve also seen the various methodologies you can use to train and fine-tune \n",
            "models efficiently and effectively. We briefly discussed prompt engineering and sampling \n",
            "techniques that greatly influence the output of an LLM, and also touched on possible \n",
            "applications of this technology. There are a number of key takeaways to keep in mind:\n",
            "Foundational Large Language Models & Text Generation70\n",
            "September 2024• The transformer architecture is the basis for all modern-day LLMs. Across the various \n",
            "architectures mentioned in this whitepaper we see that it’s important not only to add more \n",
            "parameters to the model, but the composition of the dataset is equally important. \n",
            "• The order and strategies used for fine-tuning is important and may include multiple steps \n",
            "such as Instruction Tuning, Safety Tuning, etc. Supervised Fine Tuning (SFT) is important \n",
            "in capturing the essence of a task. RLHF, and potentially RLAIF, can be used to shift the \n",
            "distribution from the pretraining distribution to a more desired one through the power of \n",
            "the reward function, that can reward desirable behaviors and penalize undesirable ones.\n",
            "• Making inference from neural models efficient is an important problem and an active \n",
            "field of research. Many methods exist to reduce serving costs and latency with minimal \n",
            "impact to model performance, and some exact acceleration methods guarantee identical \n",
            "model outputs.\n",
            "• Large language models can be used for a variety of tasks including summarization, \n",
            "translation, question answering, chat, code generation, and many more. You can \n",
            "create your own tasks using the Vertex and Makersuite text generation services which \n",
            "leverage Google’s latest language models. After the model has been trained and tuned, \n",
            "it is important to experiment with engineering prompts. You should use the technique \n",
            "most appropriate for the task-at-hand because LLMs can be sensitive to prompts k. \n",
            "Furthermore, it is also possible to enhance task specific performance or creativity and \n",
            "diversity by tweaking the parameters corresponding to sampling techniques such as \n",
            "Top-K, Top-P, and Max decoding steps to find the optimum mix of correctness, diversity, \n",
            "and creativity required for the task at hand.\n",
            "Foundational Large Language Models & Text Generation71\n",
            "September 2024Endnotes\n",
            "1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I., 2017, Attention is \n",
            "all you need. Advances in Neural Information Processing Systems , 30.\n",
            "2. Wikipedia, 2024, Word n-gram language model. Available at:  \n",
            "https://en.wikipedia.org/wiki/Word_n-gram_language_model .\n",
            "3. Sutskever, I., Vinyals, O., & Le, Q. V., 2014, Sequence to sequence learning with neural networks. Advances in \n",
            "Neural Information Processing Systems, 27.\n",
            "4. Gu, A., Goel, K., & Ré, C., 2021, Efficiently modeling long sequences with structured state spaces.  \n",
            "arXiv preprint arXiv:2111.00396.\n",
            "5. Jalammar, J. (n.d.). The illustrated transformer. Available at:  \n",
            "https://jalammar.github.io/illustrated-transformer/ .\n",
            "6. Ba, J. L., Kiros, J. R., & Hinton, G. E., 2016, Layer normalization.  \n",
            "arXiv preprint arXiv:1607.06450.\n",
            "7. He, K., Zhang, X., Ren, S., & Sun, J., 2016, Deep residual learning for image recognition. Proceedings of the \n",
            "IEEE Conference on Computer Vision and Pattern Recognition.\n",
            "8. HuggingFace., 2024, Byte Pair Encoding. Available at:  \n",
            "https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt .\n",
            "9. Kudo, T., & Richardson, J., 2018, Sentencepiece: A simple and language independent subword tokenizer and \n",
            "detokenizer for neural text processing. arXiv preprint arXiv:1808.06226.\n",
            "10. HuggingFace, 2024, Unigram tokenization. Available at:  \n",
            "https://huggingface.co/learn/nlp-course/chapter6/7?fw=pt .\n",
            "11. Goodfellow et. al., 2016, Deep Learning. MIT Press. Available at: http://www.deeplearningbook.org .\n",
            "12. Radford, Alec et al., 2019, Language models are unsupervised multitask learners.\n",
            "13. Brown, Tom, et al., 2020, Language models are few-shot learners. Advances in Neural Information \n",
            "Processing Systems , 33, 1877-1901.\n",
            "14. Devlin, Jacob, et al., 2018, BERT: Pre-training of deep bidirectional transformers for language understanding. \n",
            "arXiv preprint arXiv:1810.04805.\n",
            "Foundational Large Language Models & Text Generation72\n",
            "September 202415. Radford, A., & Narasimhan, K., 2018, Improving language understanding by generative pre-training.\n",
            "16. Dai, A., & Le, Q., 2015, Semi-supervised sequence learning. Advances in Neural Information \n",
            "Processing Systems.\n",
            "17. Ouyang, Long, et al., 2022, Training language models to follow instructions with human feedback. Advances \n",
            "in Neural Information Processing Systems, 35, 27730-27744.-27744.\n",
            "18. OpenAI., 2023, GPT-3.5. Available at: https://platform.openai.com/docs/models/gpt-3-5 .\n",
            "19. OpenAI., 2023, GPT-4 Technical Report. Available at: https://arxiv.org/abs/2303.08774 .\n",
            "20. Thoppilan, Romal, et al., 2022, Lamda: Language models for dialog applications. \n",
            "arXiv preprint arXiv:2201.08239.\n",
            "21. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models. Available \n",
            "at: https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/.\n",
            "22. Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., ... & Irving, G., 2021, Scaling language \n",
            "models: Methods, analysis & insights from training Gopher. Available at: https://arxiv.org/pdf/2112.11446.pdf .\n",
            "23. Du, N., He, H., Dai, Z., Mccarthy, J., Patwary, M. A., & Zhou, L., 2022, GLAM: Efficient scaling of language \n",
            "models with mixture-of-experts. In International Conference on Machine Learning  (pp. 2790-2800). PMLR.\n",
            "24. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D., 2020, Scaling laws \n",
            "for neural language models. arXiv preprint arXiv:2001.08361.\n",
            "25. Hoffmann, Jordan, et al., 2022, Training compute-optimal large language models. arXiv \n",
            "preprint arXiv:2203.15556.\n",
            "26. Shoeybi, Mohammad, et al., 2019, Megatron-LM: Training multi-billion parameter language models using \n",
            "model parallelism. arXiv preprint arXiv:1909.08053 .\n",
            "27. Muennighoff, N. et al., 2023, Scaling data-constrained language models. arXiv preprint arXiv:2305.16264.\n",
            "28. Chowdhery, Aakanksha, et al., 2023, Palm: Scaling language modeling with pathways. Journal of Machine \n",
            "Learning Research , 24(240), 1-113.\n",
            "29. Wang, Alex, et al.,2019, SuperGLUE: A stickier benchmark for general-purpose language understanding \n",
            "systems. Advances in Neural Information Processing Systems , 32.\n",
            "30. Anil, Rohan, et al., 2023, Palm 2 technical report. arXiv preprint arXiv:2305.10403 .\n",
            "Foundational Large Language Models & Text Generation73\n",
            "September 202431. DeepMind, 2023, Gemini: A family of highly capable multimodal models. Available at:  \n",
            "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf .\n",
            "32. DeepMind, 2024, Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. \n",
            "Available at: https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf .\n",
            "33. Google Developers, 2024, Introducing PaLi-Gemma, Gemma 2, and an upgraded responsible AI toolkit. \n",
            "Available at: https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/ .\n",
            "34. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., ... & Jegou, H., 2023, Llama 2: Open \n",
            "foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 .\n",
            "35. Jiang, A. Q., 2024, Mixtral of experts. arXiv preprint arXiv:2401.04088 .\n",
            "36. Qwen, 2024, Introducing Qwen1.5. Available at: https://qwenlm.github.io/blog/qwen1.5/ .\n",
            "37. Young, A., 2024, Yi: Open foundation models by 01.AI. arXiv preprint arXiv:2403.04652 .\n",
            "38. Grok-1, 2024, Available at: https://github.com/xai-org/grok-1 .\n",
            "39. Duan, Haodong, et al., 2023, BotChat: Evaluating LLMs’ capabilities of having multi-turn dialogues. \n",
            "arXiv preprint arXiv:2310.13650 .\n",
            "40. Google Cloud, 2024, Tune text models with reinforcement learning from human feedback. Available at:  \n",
            "https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-text-models-rlhf .\n",
            "41. Bai, Yuntao, et al., 2022, Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073 .\n",
            "42. Wikipedia, 2024, Likert scale. Available at: https://en.wikipedia.org/wiki/Likert_scale .\n",
            "43. Sutton, R. S., & Barto, A. G., 2018, Reinforcement learning: An introduction. MIT Press.\n",
            "44. Bai, Yuntao, et al, 2022, Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073 .\n",
            "45. Rafailov, Rafael, et al., 2023, Direct preference optimization: Your language model is secretly a reward \n",
            "model. arXiv preprint arXiv:2305.18290 .\n",
            "46. Houlsby, Neil, et al., 2019, Parameter-efficient transfer learning for NLP. In International Conference on \n",
            "Machine Learning  (pp. 2790-2799). PMLR.\n",
            "47. Hu, Edward J., et al., 2021, LoRA: Low-rank adaptation of large language models. \n",
            "arXiv preprint arXiv:2106.09685 .\n",
            "48. Dettmers, Tim, et al., 2023, QLoRA: Efficient finetuning of quantized LLMs. arXiv preprint arXiv:2305.14314 .\n",
            "Foundational Large Language Models & Text Generation74\n",
            "September 202449. Lester, B., Al-Rfou, R., & Constant, N., 2021, The power of scale for parameter-efficient prompt tuning. arXiv \n",
            "preprint arXiv:2104.08691 .\n",
            "50. HuggingFace., 2020, How to generate text? Available at: https://huggingface.co/blog/how-to-generate .\n",
            "51. Google AI Studio Context caching. Available \n",
            "at: https://ai.google.dev/gemini-api/docs/caching?lang=python.\n",
            "52. Vertex AI Context caching overview. Available \n",
            "at: https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview .\n",
            "53. Gu, A., Goel, K., & Ré, C., 2021, Efficiently modeling long sequences with structured state spaces.  \n",
            "Available at: https://arxiv.org/abs/2111.00396 .\n",
            "54. Hubara et al., 2016, Quantized neural networks: Training neural networks with low precision weights and \n",
            "activations. Available at: https://arxiv.org/abs/1609.07061 .\n",
            "55. Benoit Jacob et al., 2017, Quantization and training of neural networks for efficient integer-arithmetic-only \n",
            "inference. Available at: https://arxiv.org/abs/1712.05877 .\n",
            "56. Bucila, C., Caruana, R., & Niculescu-Mizil, A., 2006, Model compression. Knowledge Discovery and Data \n",
            "Mining. Available at: https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf .\n",
            "57. Hinton, G., Vinyals, O., & Dean, J., 2015, Distilling the knowledge in a neural network.  \n",
            "Available at: https://arxiv.org/abs/1503.02531 .\n",
            "58. Zhang, L., Fei, W., Wu w., He Y., Lou Z., Zhou H., 2023, Dual Grained Quantisation: Efficient Finegrained \n",
            "Quantisation for LLM. Available at: https://arxiv.org/abs/2310.04836 .\n",
            "59. Agarwal, R., Vieillard, N., Zhou, Y., Stanczyk, P., Ramos, S., Geist, M., Bachem, O., 2024, On-\n",
            "Policy Distillation of Language Models: Learning from Self-Generated Mistakes. Available \n",
            "at: https://arxiv.org/abs/2306.13649 .\n",
            "60. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J., 2017, Outrageously large neural \n",
            "networks: The sparsely-gated mixture-of-experts layer. Available at: https://arxiv.org/abs/1701.06538 .\n",
            "61. Schuster, T., Fried, D., & Jurafsky, D., 2022, Confident adaptive language modeling. Available at:  \n",
            "https://arxiv.org/abs/2207.07061 .\n",
            "62. Tri Dao et al. “FlashAttention. Available at:  \n",
            "https://arxiv.org/abs/2205.14135 .\n",
            "Foundational Large Language Models & Text Generation75\n",
            "September 202463. Leviathan, Y., Ram, O., Desbordes, T., & Haussmann, E., 2022, Fast inference from transformers via \n",
            "speculative decoding. Available at: https://arxiv.org/abs/2211.17192 .\n",
            "64. Li, Y., Humphreys, P., Sun, T., Carr, A., Cass, S., Hawkins, P., ... & Bortolussi, L., 2022, Competition-level code \n",
            "generation with AlphaCode. Science , 378(1092-1097). DOI: 10.1126/science.abq1158.\n",
            "65. Romera-Paredes, B., Barekatain, M., Novikov, A., Novikov, A., Rashed, S., & Yang, J., 2023, Mathematical \n",
            "discoveries from program search with large language models. Nature . DOI: 10.1038/s41586-023-06924-6.\n",
            "66. Wikipedia., 2024, Cap set. Available at: https://en.wikipedia.org/wiki/Cap_set .\n",
            "67. Trinh, T. H., Wu, Y., & Le, Q. V. et al., 2024, Solving olympiad geometry without human demonstrations. \n",
            "Nature, 625, 476–482. DOI: 10.1038/s41586-023-06747-5.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [pdf_text]"
      ],
      "metadata": {
        "id": "A12EIY3U0Mpj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "from google.api_core import retry\n",
        "\n",
        "\n",
        "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
        "    # Specify whether to generate embeddings for documents, or queries\n",
        "    document_mode = True\n",
        "\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        if self.document_mode:\n",
        "            embedding_task = \"retrieval_document\"\n",
        "        else:\n",
        "            embedding_task = \"retrieval_query\"\n",
        "\n",
        "        retry_policy = {\"retry\": retry.Retry(predicate=retry.if_transient_error)}\n",
        "\n",
        "        response = genai.embed_content(\n",
        "            model=\"models/text-embedding-004\",\n",
        "            content=input,\n",
        "            task_type=embedding_task,\n",
        "            request_options=retry_policy,\n",
        "        )\n",
        "        return response[\"embedding\"]"
      ],
      "metadata": {
        "id": "GETeTZ0a0SQb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "\n",
        "DB_NAME = \"googlecardb\"\n",
        "embed_fn = GeminiEmbeddingFunction()\n",
        "embed_fn.document_mode = True\n",
        "\n",
        "chroma_client = chromadb.Client()\n",
        "db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n",
        "\n",
        "db.add(documents=documents, ids=[str(i) for i in range(len(documents))])"
      ],
      "metadata": {
        "id": "fN6pGf6w0Wrj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoBNngs20at7",
        "outputId": "a5cb75c2-ee68-4d1b-f66a-9c2d7b67faae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can peek at the data too.\n",
        "#db.peek(1)"
      ],
      "metadata": {
        "id": "PuyR0XpV0dGb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "def generate_answer_from_query(db, embed_fn, query):\n",
        "    \"\"\"\n",
        "    Generates an answer from a query using Chroma DB and a generative model.\n",
        "\n",
        "    Args:\n",
        "        db: The Chroma database object to query.\n",
        "        embed_fn: The embedding function for adjusting document mode.\n",
        "        query: The user-provided question (string).\n",
        "\n",
        "    Returns:\n",
        "        str: The generated answer in Markdown format.\n",
        "    \"\"\"\n",
        "\n",
        "    # Switch to query mode when generating embeddings\n",
        "    embed_fn.document_mode = False\n",
        "\n",
        "    # Search the Chroma DB using the specified query\n",
        "    result = db.query(query_texts=[query], n_results=1)\n",
        "    [[passage]] = result[\"documents\"]\n",
        "\n",
        "    # Format the query and passage for the prompt\n",
        "    passage_oneline = passage.replace(\"\\n\", \" \")\n",
        "    query_oneline = query.replace(\"\\n\", \" \")\n",
        "\n",
        "    # Create the prompt for the generative model\n",
        "    prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below.\n",
        "    Be sure to respond in a complete sentence, being comprehensive, including all relevant background information.\n",
        "    However, you are talking to a non-technical audience, so be sure to break down complicated concepts and\n",
        "    strike a friendly and conversational tone. If the passage is irrelevant to the answer, you may ignore it.\n",
        "\n",
        "    QUESTION: {query_oneline}\n",
        "    PASSAGE: {passage_oneline}\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the generative model\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash-latest\")  # Initialize the correct model as per your setup\n",
        "\n",
        "    # Generate the answer using the generative model\n",
        "    answer = model.generate_content(prompt)\n",
        "\n",
        "    # Return the answer in Markdown format\n",
        "    return Markdown(answer.text)"
      ],
      "metadata": {
        "id": "Jwsaw8vU2ODv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `db` and `embed_fn` are initialized as per your setup:\n",
        "query = \"What can you tell me about the PaLM model?\"\n",
        "answer = generate_answer_from_query(db, embed_fn, query)\n",
        "\n",
        "# Display the result (Markdown formatting)\n",
        "display(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "Zl0ZMhWJ6rJD",
        "outputId": "062011ae-5e61-407f-c4aa-65f0994e0523"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The Pathways Language Model (PaLM) is a large language model created by Google AI.  It boasts 540 billion parameters and was trained on a massive dataset of text and code.  At the time of its release, PaLM achieved state-of-the-art results on many language benchmarks, demonstrating impressive capabilities in areas like common sense reasoning, arithmetic, joke explanation, code generation, and translation.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are some of the applications of Foundational models in Healthcare?\"\n",
        "answer = generate_answer_from_query(db, embed_fn, query)\n",
        "\n",
        "# Display the result (Markdown formatting)\n",
        "display(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "07PtffQ0_hrW",
        "outputId": "ae56f2d3-0107-46a5-d36f-afc24a86d700"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The provided text focuses on the technical aspects of foundational large language models (LLMs) and doesn't offer specific applications of these models in healthcare.  Therefore, I cannot answer your question using the given text.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What can you tell me about the evolution of transfromers in 300 words?\"\n",
        "answer = generate_answer_from_query(db, embed_fn, query)\n",
        "\n",
        "# Display the result (Markdown formatting)\n",
        "display(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "kEYkiPP8_uFw",
        "outputId": "534c2a2b-c95c-4f1a-a136-27bcfdbbe2c6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The evolution of transformers in large language models (LLMs) has been remarkable.  Initially, the transformer architecture, introduced in the \"Attention is all you need\" paper in 2017, was a sequence-to-sequence model with an encoder and decoder.  This design was used in GPT-1 (2018), which pioneered the use of unsupervised pre-training on a massive text dataset and demonstrated the potential of transformers for various tasks like text generation and translation. BERT (2018), an encoder-only model, focused on deep contextual understanding through masked language modeling.  \n\nSubsequent models like GPT-2 (2019) and GPT-3/3.5/4 (2020-2024) significantly increased in size (parameter count and training data), leading to more coherent and versatile text generation capabilities.  Google's LaMDA (2021) specialized in dialogue, while other models like Gopher and Chinchilla refined training techniques and scaling laws, focusing on dataset quality and compute efficiency.  PaLM (2022) and PaLM 2 (2023) highlighted efficient scaling using Google's Pathways system.  Finally, Google's Gemini (2023) represents the current state-of-the-art, introducing multimodality (processing various data types such as text, images, and video).  Open-source models like LLaMA and Mixtral further contributed to the field's advancement by providing accessible alternatives.  This journey showcases a continuous increase in model size and capability, coupled with advancements in training methodologies and efficiency improvements.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What can you tell me about ptompt engineering techniques?\"\n",
        "answer = generate_answer_from_query(db, embed_fn, query)\n",
        "\n",
        "# Display the result (Markdown formatting)\n",
        "display(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "0_y0SC2cAl_I",
        "outputId": "2b59a7c4-9f66-4a01-a909-b188099af767"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Prompt engineering is the art and science of crafting effective prompts to get the desired response from a large language model (LLM).  This involves techniques like providing clear instructions, offering examples (few-shot prompting), or even giving a step-by-step reasoning process (chain-of-thought prompting).  There's also zero-shot prompting, where the LLM is given only the instructions and relies on its existing knowledge.  The goal is to guide the LLM's behavior and elicit the most relevant and useful output.\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}